{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you need to do is to download this file: <a href=\"mp09.zip\">mp09.zip</a>.  It has the following content:\n",
    "\n",
    "* `submitted.py`: Your homework. Edit, and then submit to <a href=\"https://www.gradescope.com/courses/486387\">Gradescope</a>.\n",
    "* `mp09_notebook.ipynb`: This is a <a href=\"https://anaconda.org/anaconda/jupyter\">Jupyter</a> notebook to help you debug.  You can completely ignore it if you want, although you might find that it gives you useful instructions.\n",
    "* `tests`: This directory contains visible test.\n",
    "\n",
    "<!-- \n",
    "You will then need to download the CIFAR-10 dataset from the following link: <a href=\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\">CIFAR10 Dataset Download</a>. Please uncompress the zip file inside your mp09 folder. -->\n",
    "\n",
    "The list of modules you will need to import/install:\n",
    "\n",
    "* `torch`\n",
    "* `numpy`\n",
    "* `os`\n",
    "* `torchvision`\n",
    "\n",
    "\n",
    "You will not require a GPU for this MP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS440/ECE448 Spring 2023\n",
    "# MP09: Perception"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file (`mp09_notebook.ipynb`) will walk you through the full MPs with instructions and suggestion, and it is highly recommended that you follow this notebook.\n",
    "\n",
    "### Goal\n",
    "\n",
    "The objective of this assignment is to create a full end-to-end training and testing pipeline for a convolutional neural network (CNN) for the task of image classification on a modified version of the standard vision dataset `CIFAR10`. You will learn the concept of finetuning your model, in which you freeze your convolutional backbone and finetune newly initialized linear layer(s) for a specific task.\n",
    "\n",
    "There are 8 target categories: `airplane (0), automobile (1), bird (2), deer (3), frog (4), horse (5), ship (6), truck (7)`. Given an image, your CNN will be expected to successfully classify an image into one of these categories. \n",
    "\n",
    "You will be using `PyTorch` for this MP. In `MP04`, you gained some familiarity with the PyTorch library and you will build upon this foundation in this MP by designing and implementing the whole pipeline from scratch.\n",
    "\n",
    "You will need to consult the PyTorch documentation, to help you with implementation details. Please make sure you read the function definitions and descriptions in `submitted.py` carefully before completing them. \n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. <a href=\"#section1\">Dataset</a>\n",
    "1. <a href=\"#section1\">Dataloader</a>\n",
    "1. <a href=\"#section3\">Model Finetuning</a>\n",
    "1. <a href=\"#grade\">Grade Your Homework</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "<a id='section2'></a>\n",
    "## Dataset\n",
    "\n",
    "In this section you will create a PyTorch Dataset based on the `torch.utils.data.Dataset` class. \n",
    "\n",
    "Some useful resources:\n",
    "\n",
    "* <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR10 Specifications</a> for understanding the data format\n",
    "* <a href=\"https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\">Datasets and Dataloaders</a> tutorial in PyTorch\n",
    "\n",
    "It is highly recommended to read and understand these resources before diving into the code. Here is a short summary:\n",
    "\n",
    "### CIFAR10 Specifications\n",
    "\n",
    "The data folder contains 6 files, each of which is a pickled Python object: `data_batch_1`, `data_batch_2`, `data_batch_3`, `data_batch_4`, `data_batch_5`, `test_batch`. Each file contains roughly (this amount as we only observe 8 classes) 8000 samples from the dataset; the first 5 files correspond to our training set and the `test_batch` file corresponds to our test set. You will need to read all of these files and import the samples in the code. There are memory-efficient ways to do so, but simply reading all the samples from the files at once (according to train/test mode) will work fine for this assignment. Each file is a dictionary containing the data and the labels, which will be \"visible\" after unpickling. `data` is a numpy array of shape (num_samples, 3072) containing the pixel values (in range [0...255]) for the 32x32 image. The pixel values are stored in a specific order (R values, G values and then B values in row-major order) described in detail in the provided link above. `labels` is a numpy array of shape (num_samples), containing the categorical label for each sample.\n",
    "\n",
    "### Datasets and Dataloaders\n",
    "\n",
    "The `Dataset` you will need to create has three major member functions: `__init__`, `__len__`, and `__getitem__`. `__init__` is the constructor for the class inheriting `Dataset` - this is where you may want to load the data from the provided data files and store it in some member variable. `__len__` returns the length of the dataset (the number of samples) and provides the `DataLoader` wrapper with an idea of the range for index sampling. `__getitem__` should return an image and a label (a single sample) when called with a given numerical index - this function will be called many times when a batch is constructed by the `DataLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import submitted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to visualize images from https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html\n",
    "# importlib.reload(submitted)\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "# names = {0: \"ship\", 1: \"automobile\", 2: \"dog\", 3: \"frog\", 4: \"horse\"}\n",
    "# names = {0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"}\n",
    "names = {0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"deer\", 4: \"frog\", 5: \"horse\", 6: \"ship\", 7: \"truck\"}\n",
    "def show(imgs, figsize=None):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "\n",
    "    if figsize is not None:\n",
    "        fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=figsize)\n",
    "    else:\n",
    "        fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some images from one of the provided data batch files and try instantiating our data loader to see whether it works. Note that you need to complete the `build_dataset` and `build_dataloader` functions, as well as the `CIFAR10` dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(submitted)\n",
    "from torchvision import transforms\n",
    "\n",
    "# it make take a little while to build the dataset \n",
    "example_dataset = submitted.build_dataset([\"cifar10_batches/data_batch_1\"], transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of a frog\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "image, label = example_dataset[random.randint(0, len(example_dataset)-1)]\n",
    "print(\"An example of a\", names[label])\n",
    "show(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented your dataset class correctly, as well as your `build_dataset` function. You should be able to visualize random images from the dataset through the cell above. Feel free to run that cell as many times to verify that your dataset is working correctly. Now, let's instantiate a DataLoader so that we can sample batches of images from the dataset and visualize these images. Try visualizing multiple different batches. As a quick exercise, try printing out the labels of each element in the batch and verify whether it corresponds to the images correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_params = {\"batch_size\": 4, \"shuffle\": True}\n",
    "example_dataloader = submitted.build_dataloader(example_dataset, loader_params=loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image batch shape:  torch.Size([4, 3, 32, 32])\n",
      "label batch shape:  torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3200x3200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "image_batch, label_batch = next(iter(example_dataloader))\n",
    "print(\"image batch shape: \", image_batch.shape)\n",
    "print(\"label batch shape: \", label_batch.shape)\n",
    "show(make_grid([image_batch[i, :, :, :] for i in range(image_batch.shape[0])], nrows=4), figsize=(32, 32))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## Model Finetuning\n",
    "\n",
    "In this section you will create your own model based on a pretrained backbone and finetune it on the CIFAR10 dataset. \n",
    "\n",
    "Some useful resources:\n",
    "\n",
    "* <a href=\"https://pytorch.org/tutorials/beginner/saving_loading_models.html\">Saving and loading models</a> tutorial in PyTorch\n",
    "* <a href=\"https://pytorch.org/docs/master/notes/autograd.html#locally-disabling-gradient-computation\">Autograd mechanics</a> for understanding how to freeze parts of the model (go to section -- Locally disabling gradient computation)\n",
    "\n",
    "\n",
    "It is highly recommended to read and understand these resources before diving into the code. Here is a short summary:\n",
    "\n",
    "\n",
    "### Saving and loading models\n",
    "\n",
    "It is a very common and useful practice to leverage pretrained models (models that have been trained for a specific task already) on other downstream tasks. There can be many reasons to do so: (1) the representations learned by a model for a different dataset/task may transfer well to our desired task, (2) we may want to train the model with less compute resources so we don't want to train the entire model, etc. Intuitively, suppose you have trained a model on the massive ImageNet dataset to recognize all kinds of different objects and you obtain relatively good performance. The features the model has learned to extract through its convolutional backbone to detect \"general\" objects could be very applicable to a different dataset (with different objects). In this part of the MP, you will leverage a pretrained model and finetune it on our CIFAR10 dataset. We provide a model checkpoint `resnet18.pt` in the source code, corresponding to a pretrained version of the <a href=\"https://pytorch.org/hub/pytorch_vision_resnet/\">ResNet18</a> architecture. You will need to figure out how to load this model checkpoint and then identify which layers to use as your backbone (hint: only the final part of your network should be excluded from your backbone). \n",
    "\n",
    "After you load your model checkpoint, you need to also initialize new classification layers on top of the backbone. Think about what type of layers you would need for classification. Finally, you can complete your `forward` function in your network according to your logical separation of backbone/classifier. \n",
    "\n",
    "\n",
    "### Autograd mechanics\n",
    "\n",
    "You are almost done with your implementation of your network. The final step is to ensure that your backbone parameters are frozen! This means that the weight parameters in your backbone should not receive any gradient updates during backpropagation. In essence, we are assuming the backbone will already be effective for the task at hand (so it no longer needs to be trained) and we only the train the classifier. Refer to the PyTorch tutorial for help (there are also many online resources that discuss this topic).\n",
    "\n",
    "You are now ready to train and test your model! Fill out the `train` and `test` functions -- you should have a good reference in MP04 how to do so. Please be careful about the loss function / computation (e.g. if you use negative log likelihood loss, make sure your logits are normalized). `run_model` should orchestrate the entire training and testing flow and should call the functions you have completed so far.\n",
    "\n",
    "Note: ResNet18 is fairly large to train on a CPU. Do not be alarmed if it takes a few minutes to train and test. Through empirical verification, you should be able to get ~90% performance after 5-7 minutes of training if done correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models import resnet18\n",
    "\n",
    "pretrained_model = resnet18()\n",
    "print(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0\n",
      "loss:  tensor(2.0956, grad_fn=<NllLossBackward0>) ratio:  100 / 40000\n",
      "loss:  tensor(0.3741, grad_fn=<NllLossBackward0>) ratio:  200 / 40000\n",
      "loss:  tensor(0.1254, grad_fn=<NllLossBackward0>) ratio:  300 / 40000\n",
      "loss:  tensor(0.0179, grad_fn=<NllLossBackward0>) ratio:  400 / 40000\n",
      "loss:  tensor(0.0053, grad_fn=<NllLossBackward0>) ratio:  500 / 40000\n",
      "loss:  tensor(0.0268, grad_fn=<NllLossBackward0>) ratio:  600 / 40000\n",
      "loss:  tensor(0.0595, grad_fn=<NllLossBackward0>) ratio:  700 / 40000\n",
      "loss:  tensor(0.0148, grad_fn=<NllLossBackward0>) ratio:  800 / 40000\n",
      "loss:  tensor(0.1080, grad_fn=<NllLossBackward0>) ratio:  900 / 40000\n",
      "loss:  tensor(0.0047, grad_fn=<NllLossBackward0>) ratio:  1000 / 40000\n",
      "loss:  tensor(0.0663, grad_fn=<NllLossBackward0>) ratio:  1100 / 40000\n",
      "loss:  tensor(0.0346, grad_fn=<NllLossBackward0>) ratio:  1200 / 40000\n",
      "loss:  tensor(0.0388, grad_fn=<NllLossBackward0>) ratio:  1300 / 40000\n",
      "loss:  tensor(0.0225, grad_fn=<NllLossBackward0>) ratio:  1400 / 40000\n",
      "loss:  tensor(0.0003, grad_fn=<NllLossBackward0>) ratio:  1500 / 40000\n",
      "loss:  tensor(0.0112, grad_fn=<NllLossBackward0>) ratio:  1600 / 40000\n",
      "loss:  tensor(0.0055, grad_fn=<NllLossBackward0>) ratio:  1700 / 40000\n",
      "loss:  tensor(0.0150, grad_fn=<NllLossBackward0>) ratio:  1800 / 40000\n",
      "loss:  tensor(0.0012, grad_fn=<NllLossBackward0>) ratio:  1900 / 40000\n",
      "loss:  tensor(0.0283, grad_fn=<NllLossBackward0>) ratio:  2000 / 40000\n",
      "loss:  tensor(0.0115, grad_fn=<NllLossBackward0>) ratio:  2100 / 40000\n",
      "loss:  tensor(0.0456, grad_fn=<NllLossBackward0>) ratio:  2200 / 40000\n",
      "loss:  tensor(0.1121, grad_fn=<NllLossBackward0>) ratio:  2300 / 40000\n",
      "loss:  tensor(0.0021, grad_fn=<NllLossBackward0>) ratio:  2400 / 40000\n",
      "loss:  tensor(0.0003, grad_fn=<NllLossBackward0>) ratio:  2500 / 40000\n",
      "loss:  tensor(0.0181, grad_fn=<NllLossBackward0>) ratio:  2600 / 40000\n",
      "loss:  tensor(0.0010, grad_fn=<NllLossBackward0>) ratio:  2700 / 40000\n",
      "loss:  tensor(0.0161, grad_fn=<NllLossBackward0>) ratio:  2800 / 40000\n",
      "loss:  tensor(0.0018, grad_fn=<NllLossBackward0>) ratio:  2900 / 40000\n",
      "loss:  tensor(0.0008, grad_fn=<NllLossBackward0>) ratio:  3000 / 40000\n",
      "loss:  tensor(0.0026, grad_fn=<NllLossBackward0>) ratio:  3100 / 40000\n",
      "loss:  tensor(0.0406, grad_fn=<NllLossBackward0>) ratio:  3200 / 40000\n",
      "loss:  tensor(2.2985e-05, grad_fn=<NllLossBackward0>) ratio:  3300 / 40000\n",
      "loss:  tensor(0.0064, grad_fn=<NllLossBackward0>) ratio:  3400 / 40000\n",
      "loss:  tensor(0.0296, grad_fn=<NllLossBackward0>) ratio:  3500 / 40000\n",
      "loss:  tensor(0.0276, grad_fn=<NllLossBackward0>) ratio:  3600 / 40000\n",
      "loss:  tensor(0.0364, grad_fn=<NllLossBackward0>) ratio:  3700 / 40000\n",
      "loss:  tensor(0.2581, grad_fn=<NllLossBackward0>) ratio:  3800 / 40000\n",
      "loss:  tensor(0.0009, grad_fn=<NllLossBackward0>) ratio:  3900 / 40000\n",
      "loss:  tensor(3.0767e-05, grad_fn=<NllLossBackward0>) ratio:  4000 / 40000\n",
      "loss:  tensor(0.1457, grad_fn=<NllLossBackward0>) ratio:  4100 / 40000\n",
      "loss:  tensor(0.1769, grad_fn=<NllLossBackward0>) ratio:  4200 / 40000\n",
      "loss:  tensor(0.1616, grad_fn=<NllLossBackward0>) ratio:  4300 / 40000\n",
      "loss:  tensor(0.0126, grad_fn=<NllLossBackward0>) ratio:  4400 / 40000\n",
      "loss:  tensor(0.0219, grad_fn=<NllLossBackward0>) ratio:  4500 / 40000\n",
      "loss:  tensor(9.6265e-06, grad_fn=<NllLossBackward0>) ratio:  4600 / 40000\n",
      "loss:  tensor(0.0467, grad_fn=<NllLossBackward0>) ratio:  4700 / 40000\n",
      "loss:  tensor(0.0389, grad_fn=<NllLossBackward0>) ratio:  4800 / 40000\n",
      "loss:  tensor(0.0028, grad_fn=<NllLossBackward0>) ratio:  4900 / 40000\n",
      "loss:  tensor(0.1037, grad_fn=<NllLossBackward0>) ratio:  5000 / 40000\n",
      "loss:  tensor(4.6877e-05, grad_fn=<NllLossBackward0>) ratio:  5100 / 40000\n",
      "loss:  tensor(0.1547, grad_fn=<NllLossBackward0>) ratio:  5200 / 40000\n",
      "loss:  tensor(0.0993, grad_fn=<NllLossBackward0>) ratio:  5300 / 40000\n",
      "loss:  tensor(0.1232, grad_fn=<NllLossBackward0>) ratio:  5400 / 40000\n",
      "loss:  tensor(1.7691e-05, grad_fn=<NllLossBackward0>) ratio:  5500 / 40000\n",
      "loss:  tensor(0.0590, grad_fn=<NllLossBackward0>) ratio:  5600 / 40000\n",
      "loss:  tensor(0.0307, grad_fn=<NllLossBackward0>) ratio:  5700 / 40000\n",
      "loss:  tensor(0.0863, grad_fn=<NllLossBackward0>) ratio:  5800 / 40000\n",
      "loss:  tensor(0.0123, grad_fn=<NllLossBackward0>) ratio:  5900 / 40000\n",
      "loss:  tensor(0.0012, grad_fn=<NllLossBackward0>) ratio:  6000 / 40000\n",
      "loss:  tensor(0.0011, grad_fn=<NllLossBackward0>) ratio:  6100 / 40000\n",
      "loss:  tensor(0.0762, grad_fn=<NllLossBackward0>) ratio:  6200 / 40000\n",
      "loss:  tensor(8.7901e-05, grad_fn=<NllLossBackward0>) ratio:  6300 / 40000\n",
      "loss:  tensor(0.0013, grad_fn=<NllLossBackward0>) ratio:  6400 / 40000\n",
      "loss:  tensor(0.2044, grad_fn=<NllLossBackward0>) ratio:  6500 / 40000\n",
      "loss:  tensor(0.1797, grad_fn=<NllLossBackward0>) ratio:  6600 / 40000\n",
      "loss:  tensor(0.0139, grad_fn=<NllLossBackward0>) ratio:  6700 / 40000\n",
      "loss:  tensor(0.0508, grad_fn=<NllLossBackward0>) ratio:  6800 / 40000\n",
      "loss:  tensor(0.0002, grad_fn=<NllLossBackward0>) ratio:  6900 / 40000\n",
      "loss:  tensor(0.0238, grad_fn=<NllLossBackward0>) ratio:  7000 / 40000\n",
      "loss:  tensor(0.0707, grad_fn=<NllLossBackward0>) ratio:  7100 / 40000\n",
      "loss:  tensor(0.1608, grad_fn=<NllLossBackward0>) ratio:  7200 / 40000\n",
      "loss:  tensor(0.0241, grad_fn=<NllLossBackward0>) ratio:  7300 / 40000\n",
      "loss:  tensor(0.0075, grad_fn=<NllLossBackward0>) ratio:  7400 / 40000\n",
      "loss:  tensor(0.0391, grad_fn=<NllLossBackward0>) ratio:  7500 / 40000\n",
      "loss:  tensor(0.0026, grad_fn=<NllLossBackward0>) ratio:  7600 / 40000\n",
      "loss:  tensor(0.0919, grad_fn=<NllLossBackward0>) ratio:  7700 / 40000\n",
      "loss:  tensor(0.0387, grad_fn=<NllLossBackward0>) ratio:  7800 / 40000\n",
      "loss:  tensor(0.0304, grad_fn=<NllLossBackward0>) ratio:  7900 / 40000\n",
      "loss:  tensor(0.0948, grad_fn=<NllLossBackward0>) ratio:  8000 / 40000\n",
      "loss:  tensor(0.0461, grad_fn=<NllLossBackward0>) ratio:  8100 / 40000\n",
      "loss:  tensor(0.0742, grad_fn=<NllLossBackward0>) ratio:  8200 / 40000\n",
      "loss:  tensor(0.0396, grad_fn=<NllLossBackward0>) ratio:  8300 / 40000\n",
      "loss:  tensor(0.1156, grad_fn=<NllLossBackward0>) ratio:  8400 / 40000\n",
      "loss:  tensor(0.0081, grad_fn=<NllLossBackward0>) ratio:  8500 / 40000\n",
      "loss:  tensor(0.0381, grad_fn=<NllLossBackward0>) ratio:  8600 / 40000\n",
      "loss:  tensor(0.0579, grad_fn=<NllLossBackward0>) ratio:  8700 / 40000\n",
      "loss:  tensor(0.0003, grad_fn=<NllLossBackward0>) ratio:  8800 / 40000\n",
      "loss:  tensor(0.0040, grad_fn=<NllLossBackward0>) ratio:  8900 / 40000\n",
      "loss:  tensor(0.0047, grad_fn=<NllLossBackward0>) ratio:  9000 / 40000\n",
      "loss:  tensor(0.0864, grad_fn=<NllLossBackward0>) ratio:  9100 / 40000\n",
      "loss:  tensor(3.5655e-05, grad_fn=<NllLossBackward0>) ratio:  9200 / 40000\n",
      "loss:  tensor(0.0003, grad_fn=<NllLossBackward0>) ratio:  9300 / 40000\n",
      "loss:  tensor(3.5072e-05, grad_fn=<NllLossBackward0>) ratio:  9400 / 40000\n",
      "loss:  tensor(0.1761, grad_fn=<NllLossBackward0>) ratio:  9500 / 40000\n",
      "loss:  tensor(0.0054, grad_fn=<NllLossBackward0>) ratio:  9600 / 40000\n",
      "loss:  tensor(0.1866, grad_fn=<NllLossBackward0>) ratio:  9700 / 40000\n",
      "loss:  tensor(2.5939e-06, grad_fn=<NllLossBackward0>) ratio:  9800 / 40000\n",
      "loss:  tensor(0.0016, grad_fn=<NllLossBackward0>) ratio:  9900 / 40000\n",
      "loss:  tensor(0.0122, grad_fn=<NllLossBackward0>) ratio:  10000 / 40000\n",
      "loss:  tensor(0.1398, grad_fn=<NllLossBackward0>) ratio:  10100 / 40000\n",
      "loss:  tensor(0.0397, grad_fn=<NllLossBackward0>) ratio:  10200 / 40000\n",
      "loss:  tensor(0.0056, grad_fn=<NllLossBackward0>) ratio:  10300 / 40000\n",
      "loss:  tensor(0.0058, grad_fn=<NllLossBackward0>) ratio:  10400 / 40000\n",
      "loss:  tensor(1.3780e-05, grad_fn=<NllLossBackward0>) ratio:  10500 / 40000\n",
      "loss:  tensor(0.0054, grad_fn=<NllLossBackward0>) ratio:  10600 / 40000\n",
      "loss:  tensor(0.0128, grad_fn=<NllLossBackward0>) ratio:  10700 / 40000\n",
      "loss:  tensor(0.0334, grad_fn=<NllLossBackward0>) ratio:  10800 / 40000\n",
      "loss:  tensor(0.0800, grad_fn=<NllLossBackward0>) ratio:  10900 / 40000\n",
      "loss:  tensor(2.9657e-06, grad_fn=<NllLossBackward0>) ratio:  11000 / 40000\n",
      "loss:  tensor(0.0219, grad_fn=<NllLossBackward0>) ratio:  11100 / 40000\n",
      "loss:  tensor(0.0675, grad_fn=<NllLossBackward0>) ratio:  11200 / 40000\n",
      "loss:  tensor(0.0291, grad_fn=<NllLossBackward0>) ratio:  11300 / 40000\n",
      "loss:  tensor(0.0658, grad_fn=<NllLossBackward0>) ratio:  11400 / 40000\n",
      "loss:  tensor(0.0828, grad_fn=<NllLossBackward0>) ratio:  11500 / 40000\n",
      "loss:  tensor(0.0473, grad_fn=<NllLossBackward0>) ratio:  11600 / 40000\n",
      "loss:  tensor(0.0026, grad_fn=<NllLossBackward0>) ratio:  11700 / 40000\n",
      "loss:  tensor(0.1020, grad_fn=<NllLossBackward0>) ratio:  11800 / 40000\n",
      "loss:  tensor(0.0638, grad_fn=<NllLossBackward0>) ratio:  11900 / 40000\n",
      "loss:  tensor(0.0186, grad_fn=<NllLossBackward0>) ratio:  12000 / 40000\n",
      "loss:  tensor(0.1957, grad_fn=<NllLossBackward0>) ratio:  12100 / 40000\n",
      "loss:  tensor(0.0288, grad_fn=<NllLossBackward0>) ratio:  12200 / 40000\n",
      "loss:  tensor(0.0350, grad_fn=<NllLossBackward0>) ratio:  12300 / 40000\n",
      "loss:  tensor(0.0547, grad_fn=<NllLossBackward0>) ratio:  12400 / 40000\n",
      "loss:  tensor(0.1162, grad_fn=<NllLossBackward0>) ratio:  12500 / 40000\n",
      "loss:  tensor(2.0259e-05, grad_fn=<NllLossBackward0>) ratio:  12600 / 40000\n",
      "loss:  tensor(0.0041, grad_fn=<NllLossBackward0>) ratio:  12700 / 40000\n",
      "loss:  tensor(5.3695e-05, grad_fn=<NllLossBackward0>) ratio:  12800 / 40000\n",
      "loss:  tensor(0.1543, grad_fn=<NllLossBackward0>) ratio:  12900 / 40000\n",
      "loss:  tensor(0.2223, grad_fn=<NllLossBackward0>) ratio:  13000 / 40000\n",
      "loss:  tensor(0.0514, grad_fn=<NllLossBackward0>) ratio:  13100 / 40000\n",
      "loss:  tensor(0.0205, grad_fn=<NllLossBackward0>) ratio:  13200 / 40000\n",
      "loss:  tensor(0.1406, grad_fn=<NllLossBackward0>) ratio:  13300 / 40000\n",
      "loss:  tensor(0.0021, grad_fn=<NllLossBackward0>) ratio:  13400 / 40000\n",
      "loss:  tensor(0.0520, grad_fn=<NllLossBackward0>) ratio:  13500 / 40000\n",
      "loss:  tensor(0.0232, grad_fn=<NllLossBackward0>) ratio:  13600 / 40000\n",
      "loss:  tensor(0.0063, grad_fn=<NllLossBackward0>) ratio:  13700 / 40000\n",
      "loss:  tensor(0.0044, grad_fn=<NllLossBackward0>) ratio:  13800 / 40000\n",
      "loss:  tensor(0.0419, grad_fn=<NllLossBackward0>) ratio:  13900 / 40000\n",
      "loss:  tensor(0.0001, grad_fn=<NllLossBackward0>) ratio:  14000 / 40000\n",
      "loss:  tensor(0.0459, grad_fn=<NllLossBackward0>) ratio:  14100 / 40000\n",
      "loss:  tensor(0.1744, grad_fn=<NllLossBackward0>) ratio:  14200 / 40000\n",
      "loss:  tensor(0.0887, grad_fn=<NllLossBackward0>) ratio:  14300 / 40000\n",
      "loss:  tensor(0.0432, grad_fn=<NllLossBackward0>) ratio:  14400 / 40000\n",
      "loss:  tensor(0.0862, grad_fn=<NllLossBackward0>) ratio:  14500 / 40000\n",
      "loss:  tensor(0.0096, grad_fn=<NllLossBackward0>) ratio:  14600 / 40000\n",
      "loss:  tensor(0.0826, grad_fn=<NllLossBackward0>) ratio:  14700 / 40000\n",
      "loss:  tensor(0.0002, grad_fn=<NllLossBackward0>) ratio:  14800 / 40000\n",
      "loss:  tensor(0.0138, grad_fn=<NllLossBackward0>) ratio:  14900 / 40000\n",
      "loss:  tensor(0.1161, grad_fn=<NllLossBackward0>) ratio:  15000 / 40000\n",
      "loss:  tensor(0.0896, grad_fn=<NllLossBackward0>) ratio:  15100 / 40000\n",
      "loss:  tensor(0.0194, grad_fn=<NllLossBackward0>) ratio:  15200 / 40000\n",
      "loss:  tensor(0.0551, grad_fn=<NllLossBackward0>) ratio:  15300 / 40000\n",
      "loss:  tensor(0.0017, grad_fn=<NllLossBackward0>) ratio:  15400 / 40000\n",
      "loss:  tensor(0.0505, grad_fn=<NllLossBackward0>) ratio:  15500 / 40000\n",
      "loss:  tensor(0.0114, grad_fn=<NllLossBackward0>) ratio:  15600 / 40000\n",
      "loss:  tensor(0.2806, grad_fn=<NllLossBackward0>) ratio:  15700 / 40000\n",
      "loss:  tensor(0.0059, grad_fn=<NllLossBackward0>) ratio:  15800 / 40000\n",
      "loss:  tensor(2.2897e-05, grad_fn=<NllLossBackward0>) ratio:  15900 / 40000\n",
      "loss:  tensor(0.0684, grad_fn=<NllLossBackward0>) ratio:  16000 / 40000\n",
      "loss:  tensor(0.0679, grad_fn=<NllLossBackward0>) ratio:  16100 / 40000\n",
      "loss:  tensor(0.1018, grad_fn=<NllLossBackward0>) ratio:  16200 / 40000\n",
      "loss:  tensor(0.0002, grad_fn=<NllLossBackward0>) ratio:  16300 / 40000\n",
      "loss:  tensor(0.0061, grad_fn=<NllLossBackward0>) ratio:  16400 / 40000\n",
      "loss:  tensor(0.0174, grad_fn=<NllLossBackward0>) ratio:  16500 / 40000\n",
      "loss:  tensor(0.0682, grad_fn=<NllLossBackward0>) ratio:  16600 / 40000\n",
      "loss:  tensor(0.0344, grad_fn=<NllLossBackward0>) ratio:  16700 / 40000\n",
      "loss:  tensor(0.0009, grad_fn=<NllLossBackward0>) ratio:  16800 / 40000\n",
      "loss:  tensor(0.0348, grad_fn=<NllLossBackward0>) ratio:  16900 / 40000\n",
      "loss:  tensor(0.0045, grad_fn=<NllLossBackward0>) ratio:  17000 / 40000\n",
      "loss:  tensor(0.1185, grad_fn=<NllLossBackward0>) ratio:  17100 / 40000\n",
      "loss:  tensor(0.0071, grad_fn=<NllLossBackward0>) ratio:  17200 / 40000\n",
      "loss:  tensor(0.0004, grad_fn=<NllLossBackward0>) ratio:  17300 / 40000\n",
      "loss:  tensor(0.0318, grad_fn=<NllLossBackward0>) ratio:  17400 / 40000\n",
      "loss:  tensor(3.7045e-05, grad_fn=<NllLossBackward0>) ratio:  17500 / 40000\n",
      "loss:  tensor(0.1004, grad_fn=<NllLossBackward0>) ratio:  17600 / 40000\n",
      "loss:  tensor(0.0314, grad_fn=<NllLossBackward0>) ratio:  17700 / 40000\n",
      "loss:  tensor(0.0002, grad_fn=<NllLossBackward0>) ratio:  17800 / 40000\n",
      "loss:  tensor(0.0015, grad_fn=<NllLossBackward0>) ratio:  17900 / 40000\n",
      "loss:  tensor(0.0556, grad_fn=<NllLossBackward0>) ratio:  18000 / 40000\n",
      "loss:  tensor(0.1130, grad_fn=<NllLossBackward0>) ratio:  18100 / 40000\n",
      "loss:  tensor(0.1051, grad_fn=<NllLossBackward0>) ratio:  18200 / 40000\n",
      "loss:  tensor(0.0020, grad_fn=<NllLossBackward0>) ratio:  18300 / 40000\n",
      "loss:  tensor(0.1201, grad_fn=<NllLossBackward0>) ratio:  18400 / 40000\n",
      "loss:  tensor(0.0007, grad_fn=<NllLossBackward0>) ratio:  18500 / 40000\n",
      "loss:  tensor(0.0386, grad_fn=<NllLossBackward0>) ratio:  18600 / 40000\n",
      "loss:  tensor(0.0009, grad_fn=<NllLossBackward0>) ratio:  18700 / 40000\n",
      "loss:  tensor(0.0035, grad_fn=<NllLossBackward0>) ratio:  18800 / 40000\n",
      "loss:  tensor(4.7496e-06, grad_fn=<NllLossBackward0>) ratio:  18900 / 40000\n",
      "loss:  tensor(8.4305e-05, grad_fn=<NllLossBackward0>) ratio:  19000 / 40000\n",
      "loss:  tensor(0.0959, grad_fn=<NllLossBackward0>) ratio:  19100 / 40000\n",
      "loss:  tensor(0.0875, grad_fn=<NllLossBackward0>) ratio:  19200 / 40000\n",
      "loss:  tensor(0.1282, grad_fn=<NllLossBackward0>) ratio:  19300 / 40000\n",
      "loss:  tensor(0.0666, grad_fn=<NllLossBackward0>) ratio:  19400 / 40000\n",
      "loss:  tensor(0.2334, grad_fn=<NllLossBackward0>) ratio:  19500 / 40000\n",
      "loss:  tensor(0.0068, grad_fn=<NllLossBackward0>) ratio:  19600 / 40000\n",
      "loss:  tensor(0.0948, grad_fn=<NllLossBackward0>) ratio:  19700 / 40000\n",
      "loss:  tensor(0.0018, grad_fn=<NllLossBackward0>) ratio:  19800 / 40000\n",
      "loss:  tensor(0.0517, grad_fn=<NllLossBackward0>) ratio:  19900 / 40000\n",
      "loss:  tensor(0.0448, grad_fn=<NllLossBackward0>) ratio:  20000 / 40000\n",
      "loss:  tensor(9.5277e-05, grad_fn=<NllLossBackward0>) ratio:  20100 / 40000\n",
      "loss:  tensor(0.0809, grad_fn=<NllLossBackward0>) ratio:  20200 / 40000\n",
      "loss:  tensor(0.1269, grad_fn=<NllLossBackward0>) ratio:  20300 / 40000\n",
      "loss:  tensor(0.1159, grad_fn=<NllLossBackward0>) ratio:  20400 / 40000\n",
      "loss:  tensor(0.0102, grad_fn=<NllLossBackward0>) ratio:  20500 / 40000\n",
      "loss:  tensor(0.0002, grad_fn=<NllLossBackward0>) ratio:  20600 / 40000\n",
      "loss:  tensor(0.0812, grad_fn=<NllLossBackward0>) ratio:  20700 / 40000\n",
      "loss:  tensor(0.1417, grad_fn=<NllLossBackward0>) ratio:  20800 / 40000\n",
      "loss:  tensor(0.0937, grad_fn=<NllLossBackward0>) ratio:  20900 / 40000\n",
      "loss:  tensor(0.0323, grad_fn=<NllLossBackward0>) ratio:  21000 / 40000\n",
      "loss:  tensor(0.1059, grad_fn=<NllLossBackward0>) ratio:  21100 / 40000\n",
      "loss:  tensor(0.0808, grad_fn=<NllLossBackward0>) ratio:  21200 / 40000\n",
      "loss:  tensor(0.1339, grad_fn=<NllLossBackward0>) ratio:  21300 / 40000\n",
      "loss:  tensor(0.0236, grad_fn=<NllLossBackward0>) ratio:  21400 / 40000\n",
      "loss:  tensor(0.0005, grad_fn=<NllLossBackward0>) ratio:  21500 / 40000\n",
      "loss:  tensor(0.0450, grad_fn=<NllLossBackward0>) ratio:  21600 / 40000\n",
      "loss:  tensor(0.0006, grad_fn=<NllLossBackward0>) ratio:  21700 / 40000\n",
      "loss:  tensor(0.0030, grad_fn=<NllLossBackward0>) ratio:  21800 / 40000\n",
      "loss:  tensor(0.0069, grad_fn=<NllLossBackward0>) ratio:  21900 / 40000\n",
      "loss:  tensor(0.0842, grad_fn=<NllLossBackward0>) ratio:  22000 / 40000\n",
      "loss:  tensor(0.0497, grad_fn=<NllLossBackward0>) ratio:  22100 / 40000\n",
      "loss:  tensor(0.0203, grad_fn=<NllLossBackward0>) ratio:  22200 / 40000\n",
      "loss:  tensor(0.0753, grad_fn=<NllLossBackward0>) ratio:  22300 / 40000\n",
      "loss:  tensor(0.0009, grad_fn=<NllLossBackward0>) ratio:  22400 / 40000\n",
      "loss:  tensor(0.0802, grad_fn=<NllLossBackward0>) ratio:  22500 / 40000\n",
      "loss:  tensor(0.0459, grad_fn=<NllLossBackward0>) ratio:  22600 / 40000\n",
      "loss:  tensor(0.0024, grad_fn=<NllLossBackward0>) ratio:  22700 / 40000\n",
      "loss:  tensor(0.0111, grad_fn=<NllLossBackward0>) ratio:  22800 / 40000\n",
      "loss:  tensor(0.0011, grad_fn=<NllLossBackward0>) ratio:  22900 / 40000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m importlib\u001b[39m.\u001b[39mreload(submitted)\n\u001b[0;32m----> 2\u001b[0m submitted\u001b[39m.\u001b[39;49mrun_model()\n",
      "File \u001b[0;32m~/Desktop/ECE 448/AI_Repo/mp09/submitted.py:310\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m    309\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch #\u001b[39m\u001b[39m\"\u001b[39m, epoch)\n\u001b[0;32m--> 310\u001b[0m     train(train_loader, model, loss_fn, optimizer)  \u001b[39m# You need to write this function.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39m# test(test_dataloader, model, loss_fn)  # optional, to monitor the training progress\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Desktop/ECE 448/AI_Repo/mp09/submitted.py:241\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39m################# Your Code Starts Here #################\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39mfor\u001b[39;00m features, labels \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[0;32m--> 241\u001b[0m     y_pred \u001b[39m=\u001b[39m model(features)\n\u001b[1;32m    242\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(y_pred, labels)\n\u001b[1;32m    243\u001b[0m     current \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m y_pred\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ECE 448/AI_Repo/mp09/submitted.py:177\u001b[0m, in \u001b[0;36mFinetuneNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mPerform a forward pass through your neural net.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39m    y:      an (N, output_size) tensor of output from the network\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m################# Your Code Starts Here #################\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39m# resize / reshape x to ResNet18 \u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_model(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ECE 448/AI_Repo/mp09/models.py:253\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    250\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[1;32m    252\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 253\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(x)\n\u001b[1;32m    254\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[1;32m    255\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ECE 448/AI_Repo/mp09/models.py:67\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     65\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m---> 67\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     68\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m     69\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "submitted.run_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grade'></a>\n",
    "## Grade your homework\n",
    "\n",
    "If you've reached this point, and all of the above sections work, then you're ready to try grading your homework! Before you submit it to Gradescope, try grading it on your own machine. This will run some visible test cases. Note that these visible test cases do not test the accuracy of your model, but we expect your finetuned model to achieve at least 90% accuracy on the test set to pass the hidden test cases. Make sure you test locally and can ensure that you can achieve 90% accuracy before submitting to the autograder (it may take even longer to run on the autograder).\n",
    "\n",
    "The exclamation point (!) tells python to run the following as a shell command. Obviously you don't need to run the code this way -- this usage is here just to remind you that you can also, if you wish, run this command in a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradescope_utils in /Users/pranavsriram/opt/anaconda3/lib/python3.8/site-packages (0.5.0)\n",
      "Datasets built.\n",
      "Dataloaders built.\n",
      "Model built. Starting training....\n",
      "Train epoch 0: Loss: 2.0863759517669678, 0 / 39995 [0.0%]\n",
      "Train epoch 0: Loss: 2.0318400859832764, 64 / 39995 [0.16%]\n",
      "Train epoch 0: Loss: 1.9658395051956177, 128 / 39995 [0.32%]\n",
      "Train epoch 0: Loss: 1.904288649559021, 192 / 39995 [0.48%]\n",
      "Train epoch 0: Loss: 1.8394948244094849, 256 / 39995 [0.64%]\n",
      "Train epoch 0: Loss: 1.7957719564437866, 320 / 39995 [0.8%]\n",
      "Train epoch 0: Loss: 1.7265615463256836, 384 / 39995 [0.96%]\n",
      "Train epoch 0: Loss: 1.6867951154708862, 448 / 39995 [1.12%]\n",
      "Train epoch 0: Loss: 1.637083888053894, 512 / 39995 [1.28%]\n",
      "Train epoch 0: Loss: 1.5728893280029297, 576 / 39995 [1.44%]\n",
      "Train epoch 0: Loss: 1.5163304805755615, 640 / 39995 [1.6%]\n",
      "Train epoch 0: Loss: 1.4553488492965698, 704 / 39995 [1.76%]\n",
      "Train epoch 0: Loss: 1.415666103363037, 768 / 39995 [1.92%]\n",
      "Train epoch 0: Loss: 1.3699871301651, 832 / 39995 [2.08%]\n",
      "Train epoch 0: Loss: 1.3550962209701538, 896 / 39995 [2.24%]\n",
      "Train epoch 0: Loss: 1.268727421760559, 960 / 39995 [2.4%]\n",
      "Train epoch 0: Loss: 1.2014806270599365, 1024 / 39995 [2.56%]\n",
      "Train epoch 0: Loss: 1.1739234924316406, 1088 / 39995 [2.72%]\n",
      "Train epoch 0: Loss: 1.1174687147140503, 1152 / 39995 [2.88%]\n",
      "Train epoch 0: Loss: 1.0864241123199463, 1216 / 39995 [3.04%]\n",
      "Train epoch 0: Loss: 1.0383265018463135, 1280 / 39995 [3.2%]\n",
      "Train epoch 0: Loss: 1.0637120008468628, 1344 / 39995 [3.36%]\n",
      "Train epoch 0: Loss: 1.0138152837753296, 1408 / 39995 [3.52%]\n",
      "Train epoch 0: Loss: 0.9663822054862976, 1472 / 39995 [3.68%]\n",
      "Train epoch 0: Loss: 0.9143064618110657, 1536 / 39995 [3.84%]\n",
      "Train epoch 0: Loss: 0.8723706007003784, 1600 / 39995 [4.0%]\n",
      "Train epoch 0: Loss: 0.8784564137458801, 1664 / 39995 [4.16%]\n",
      "Train epoch 0: Loss: 0.8058015704154968, 1728 / 39995 [4.32%]\n",
      "Train epoch 0: Loss: 0.8167240619659424, 1792 / 39995 [4.48%]\n",
      "Train epoch 0: Loss: 0.8027297258377075, 1856 / 39995 [4.64%]\n",
      "Train epoch 0: Loss: 0.7092670202255249, 1920 / 39995 [4.8%]\n",
      "Train epoch 0: Loss: 0.7079779505729675, 1984 / 39995 [4.96%]\n",
      "Train epoch 0: Loss: 0.7586937546730042, 2048 / 39995 [5.12%]\n",
      "Train epoch 0: Loss: 0.7168750762939453, 2112 / 39995 [5.28%]\n",
      "Train epoch 0: Loss: 0.6061670780181885, 2176 / 39995 [5.44%]\n",
      "Train epoch 0: Loss: 0.5901877880096436, 2240 / 39995 [5.6%]\n",
      "Train epoch 0: Loss: 0.557778000831604, 2304 / 39995 [5.76%]\n",
      "Train epoch 0: Loss: 0.6043642163276672, 2368 / 39995 [5.92%]\n",
      "Train epoch 0: Loss: 0.5295153856277466, 2432 / 39995 [6.08%]\n",
      "Train epoch 0: Loss: 0.5028281211853027, 2496 / 39995 [6.24%]\n",
      "Train epoch 0: Loss: 0.5240253210067749, 2560 / 39995 [6.4%]\n",
      "Train epoch 0: Loss: 0.4970299005508423, 2624 / 39995 [6.56%]\n",
      "Train epoch 0: Loss: 0.45811939239501953, 2688 / 39995 [6.72%]\n",
      "Train epoch 0: Loss: 0.5003327131271362, 2752 / 39995 [6.88%]\n",
      "Train epoch 0: Loss: 0.47085922956466675, 2816 / 39995 [7.04%]\n",
      "Train epoch 0: Loss: 0.4660385549068451, 2880 / 39995 [7.2%]\n",
      "Train epoch 0: Loss: 0.37427398562431335, 2944 / 39995 [7.36%]\n",
      "Train epoch 0: Loss: 0.4047382175922394, 3008 / 39995 [7.52%]\n",
      "Train epoch 0: Loss: 0.39659109711647034, 3072 / 39995 [7.68%]\n",
      "Train epoch 0: Loss: 0.4223729074001312, 3136 / 39995 [7.84%]\n",
      "Train epoch 0: Loss: 0.3734342157840729, 3200 / 39995 [8.0%]\n",
      "Train epoch 0: Loss: 0.4631831645965576, 3264 / 39995 [8.16%]\n",
      "Train epoch 0: Loss: 0.3967108130455017, 3328 / 39995 [8.32%]\n",
      "Train epoch 0: Loss: 0.31680965423583984, 3392 / 39995 [8.48%]\n",
      "Train epoch 0: Loss: 0.31321749091148376, 3456 / 39995 [8.64%]\n",
      "Train epoch 0: Loss: 0.33348390460014343, 3520 / 39995 [8.8%]\n",
      "Train epoch 0: Loss: 0.37877246737480164, 3584 / 39995 [8.96%]\n",
      "Train epoch 0: Loss: 0.34892141819000244, 3648 / 39995 [9.12%]\n",
      "Train epoch 0: Loss: 0.3436005711555481, 3712 / 39995 [9.28%]\n",
      "Train epoch 0: Loss: 0.31271982192993164, 3776 / 39995 [9.44%]\n",
      "Train epoch 0: Loss: 0.3047443628311157, 3840 / 39995 [9.6%]\n",
      "Train epoch 0: Loss: 0.37003064155578613, 3904 / 39995 [9.76%]\n",
      "Train epoch 0: Loss: 0.2669072449207306, 3968 / 39995 [9.92%]\n",
      "Train epoch 0: Loss: 0.2991431653499603, 4032 / 39995 [10.08%]\n",
      "Train epoch 0: Loss: 0.2158452570438385, 4096 / 39995 [10.24%]\n",
      "Train epoch 0: Loss: 0.2759068012237549, 4160 / 39995 [10.4%]\n",
      "Train epoch 0: Loss: 0.3804832994937897, 4224 / 39995 [10.56%]\n",
      "Train epoch 0: Loss: 0.23616887629032135, 4288 / 39995 [10.72%]\n",
      "Train epoch 0: Loss: 0.255097895860672, 4352 / 39995 [10.88%]\n",
      "Train epoch 0: Loss: 0.22264881432056427, 4416 / 39995 [11.04%]\n",
      "Train epoch 0: Loss: 0.2546139061450958, 4480 / 39995 [11.2%]\n",
      "Train epoch 0: Loss: 0.2605377733707428, 4544 / 39995 [11.36%]\n",
      "Train epoch 0: Loss: 0.22995837032794952, 4608 / 39995 [11.52%]\n",
      "Train epoch 0: Loss: 0.24688923358917236, 4672 / 39995 [11.68%]\n",
      "Train epoch 0: Loss: 0.180977001786232, 4736 / 39995 [11.84%]\n",
      "Train epoch 0: Loss: 0.19915592670440674, 4800 / 39995 [12.0%]\n",
      "Train epoch 0: Loss: 0.19306200742721558, 4864 / 39995 [12.16%]\n",
      "Train epoch 0: Loss: 0.2274877429008484, 4928 / 39995 [12.32%]\n",
      "Train epoch 0: Loss: 0.20151232182979584, 4992 / 39995 [12.48%]\n",
      "Train epoch 0: Loss: 0.18980318307876587, 5056 / 39995 [12.64%]\n",
      "Train epoch 0: Loss: 0.15840354561805725, 5120 / 39995 [12.8%]\n",
      "Train epoch 0: Loss: 0.28871017694473267, 5184 / 39995 [12.96%]\n",
      "Train epoch 0: Loss: 0.16815197467803955, 5248 / 39995 [13.12%]\n",
      "Train epoch 0: Loss: 0.1977928876876831, 5312 / 39995 [13.28%]\n",
      "Train epoch 0: Loss: 0.1639438420534134, 5376 / 39995 [13.44%]\n",
      "Train epoch 0: Loss: 0.1656763106584549, 5440 / 39995 [13.6%]\n",
      "Train epoch 0: Loss: 0.19729967415332794, 5504 / 39995 [13.76%]\n",
      "Train epoch 0: Loss: 0.18935924768447876, 5568 / 39995 [13.92%]\n",
      "Train epoch 0: Loss: 0.16716361045837402, 5632 / 39995 [14.08%]\n",
      "Train epoch 0: Loss: 0.2133297324180603, 5696 / 39995 [14.24%]\n",
      "Train epoch 0: Loss: 0.20249691605567932, 5760 / 39995 [14.4%]\n",
      "Train epoch 0: Loss: 0.1578662097454071, 5824 / 39995 [14.56%]\n",
      "Train epoch 0: Loss: 0.18447613716125488, 5888 / 39995 [14.72%]\n",
      "Train epoch 0: Loss: 0.11972524970769882, 5952 / 39995 [14.88%]\n",
      "Train epoch 0: Loss: 0.15639807283878326, 6016 / 39995 [15.04%]\n",
      "Train epoch 0: Loss: 0.15316228568553925, 6080 / 39995 [15.2%]\n",
      "Train epoch 0: Loss: 0.14387373626232147, 6144 / 39995 [15.36%]\n",
      "Train epoch 0: Loss: 0.21912415325641632, 6208 / 39995 [15.52%]\n",
      "Train epoch 0: Loss: 0.12899234890937805, 6272 / 39995 [15.68%]\n",
      "Train epoch 0: Loss: 0.16415217518806458, 6336 / 39995 [15.84%]\n",
      "Train epoch 0: Loss: 0.13120737671852112, 6400 / 39995 [16.0%]\n",
      "Train epoch 0: Loss: 0.13463804125785828, 6464 / 39995 [16.16%]\n",
      "Train epoch 0: Loss: 0.18624679744243622, 6528 / 39995 [16.32%]\n",
      "Train epoch 0: Loss: 0.17932887375354767, 6592 / 39995 [16.48%]\n",
      "Train epoch 0: Loss: 0.14044325053691864, 6656 / 39995 [16.64%]\n",
      "Train epoch 0: Loss: 0.1496724635362625, 6720 / 39995 [16.8%]\n",
      "Train epoch 0: Loss: 0.20452924072742462, 6784 / 39995 [16.96%]\n",
      "Train epoch 0: Loss: 0.14291414618492126, 6848 / 39995 [17.12%]\n",
      "Train epoch 0: Loss: 0.13083666563034058, 6912 / 39995 [17.28%]\n",
      "Train epoch 0: Loss: 0.12226424366235733, 6976 / 39995 [17.44%]\n",
      "Train epoch 0: Loss: 0.1404186189174652, 7040 / 39995 [17.6%]\n",
      "Train epoch 0: Loss: 0.11219566315412521, 7104 / 39995 [17.76%]\n",
      "Train epoch 0: Loss: 0.10215923935174942, 7168 / 39995 [17.92%]\n",
      "Train epoch 0: Loss: 0.1571601927280426, 7232 / 39995 [18.08%]\n",
      "Train epoch 0: Loss: 0.10984201729297638, 7296 / 39995 [18.24%]\n",
      "Train epoch 0: Loss: 0.19591642916202545, 7360 / 39995 [18.4%]\n",
      "Train epoch 0: Loss: 0.1823619157075882, 7424 / 39995 [18.56%]\n",
      "Train epoch 0: Loss: 0.09698609262704849, 7488 / 39995 [18.72%]\n",
      "Train epoch 0: Loss: 0.09959348291158676, 7552 / 39995 [18.88%]\n",
      "Train epoch 0: Loss: 0.14614032208919525, 7616 / 39995 [19.04%]\n",
      "Train epoch 0: Loss: 0.10757801681756973, 7680 / 39995 [19.2%]\n",
      "Train epoch 0: Loss: 0.08430547267198563, 7744 / 39995 [19.36%]\n",
      "Train epoch 0: Loss: 0.12847909331321716, 7808 / 39995 [19.52%]\n",
      "Train epoch 0: Loss: 0.10641826689243317, 7872 / 39995 [19.68%]\n",
      "Train epoch 0: Loss: 0.14391104876995087, 7936 / 39995 [19.84%]\n",
      "Train epoch 0: Loss: 0.14535216987133026, 8000 / 39995 [20.0%]\n",
      "Train epoch 0: Loss: 0.16260512173175812, 8064 / 39995 [20.16%]\n",
      "Train epoch 0: Loss: 0.0810110867023468, 8128 / 39995 [20.32%]\n",
      "Train epoch 0: Loss: 0.10373825579881668, 8192 / 39995 [20.48%]\n",
      "Train epoch 0: Loss: 0.10247980803251266, 8256 / 39995 [20.64%]\n",
      "Train epoch 0: Loss: 0.11073277145624161, 8320 / 39995 [20.8%]\n",
      "Train epoch 0: Loss: 0.11144290119409561, 8384 / 39995 [20.96%]\n",
      "Train epoch 0: Loss: 0.1821364313364029, 8448 / 39995 [21.12%]\n",
      "Train epoch 0: Loss: 0.1215064749121666, 8512 / 39995 [21.28%]\n",
      "Train epoch 0: Loss: 0.1033942773938179, 8576 / 39995 [21.44%]\n",
      "Train epoch 0: Loss: 0.14544755220413208, 8640 / 39995 [21.6%]\n",
      "Train epoch 0: Loss: 0.1865772157907486, 8704 / 39995 [21.76%]\n",
      "Train epoch 0: Loss: 0.1104646846652031, 8768 / 39995 [21.92%]\n",
      "Train epoch 0: Loss: 0.09105606377124786, 8832 / 39995 [22.08%]\n",
      "Train epoch 0: Loss: 0.16210158169269562, 8896 / 39995 [22.24%]\n",
      "Train epoch 0: Loss: 0.1111006960272789, 8960 / 39995 [22.4%]\n",
      "Train epoch 0: Loss: 0.11198535561561584, 9024 / 39995 [22.56%]\n",
      "Train epoch 0: Loss: 0.1697636842727661, 9088 / 39995 [22.72%]\n",
      "Train epoch 0: Loss: 0.1168472021818161, 9152 / 39995 [22.88%]\n",
      "Train epoch 0: Loss: 0.10438816249370575, 9216 / 39995 [23.04%]\n",
      "Train epoch 0: Loss: 0.08532126992940903, 9280 / 39995 [23.2%]\n",
      "Train epoch 0: Loss: 0.135142982006073, 9344 / 39995 [23.36%]\n",
      "Train epoch 0: Loss: 0.10319584608078003, 9408 / 39995 [23.52%]\n",
      "Train epoch 0: Loss: 0.15600185096263885, 9472 / 39995 [23.68%]\n",
      "Train epoch 0: Loss: 0.05918395146727562, 9536 / 39995 [23.84%]\n",
      "Train epoch 0: Loss: 0.0887330174446106, 9600 / 39995 [24.0%]\n",
      "Train epoch 0: Loss: 0.11879169195890427, 9664 / 39995 [24.16%]\n",
      "Train epoch 0: Loss: 0.05693349242210388, 9728 / 39995 [24.32%]\n",
      "Train epoch 0: Loss: 0.09432309865951538, 9792 / 39995 [24.48%]\n",
      "Train epoch 0: Loss: 0.08160721510648727, 9856 / 39995 [24.64%]\n",
      "Train epoch 0: Loss: 0.0799853503704071, 9920 / 39995 [24.8%]\n",
      "Train epoch 0: Loss: 0.10014109313488007, 9984 / 39995 [24.96%]\n",
      "Train epoch 0: Loss: 0.10683683305978775, 10048 / 39995 [25.12%]\n",
      "Train epoch 0: Loss: 0.07078266888856888, 10112 / 39995 [25.28%]\n",
      "Train epoch 0: Loss: 0.09036482870578766, 10176 / 39995 [25.44%]\n",
      "Train epoch 0: Loss: 0.11010279506444931, 10240 / 39995 [25.6%]\n",
      "Train epoch 0: Loss: 0.08363151550292969, 10304 / 39995 [25.76%]\n",
      "Train epoch 0: Loss: 0.1634581834077835, 10368 / 39995 [25.92%]\n",
      "Train epoch 0: Loss: 0.08855950087308884, 10432 / 39995 [26.08%]\n",
      "Train epoch 0: Loss: 0.1201813817024231, 10496 / 39995 [26.24%]\n",
      "Train epoch 0: Loss: 0.19203993678092957, 10560 / 39995 [26.4%]\n",
      "Train epoch 0: Loss: 0.06787681579589844, 10624 / 39995 [26.56%]\n",
      "Train epoch 0: Loss: 0.09349482506513596, 10688 / 39995 [26.72%]\n",
      "Train epoch 0: Loss: 0.1017167866230011, 10752 / 39995 [26.88%]\n",
      "Train epoch 0: Loss: 0.059506628662347794, 10816 / 39995 [27.04%]\n",
      "Train epoch 0: Loss: 0.07223408669233322, 10880 / 39995 [27.2%]\n",
      "Train epoch 0: Loss: 0.05740614980459213, 10944 / 39995 [27.36%]\n",
      "Train epoch 0: Loss: 0.08809440582990646, 11008 / 39995 [27.52%]\n",
      "Train epoch 0: Loss: 0.08129480481147766, 11072 / 39995 [27.68%]\n",
      "Train epoch 0: Loss: 0.06232025474309921, 11136 / 39995 [27.84%]\n",
      "Train epoch 0: Loss: 0.10280216485261917, 11200 / 39995 [28.0%]\n",
      "Train epoch 0: Loss: 0.09733407944440842, 11264 / 39995 [28.16%]\n",
      "Train epoch 0: Loss: 0.05140114575624466, 11328 / 39995 [28.32%]\n",
      "Train epoch 0: Loss: 0.09939821064472198, 11392 / 39995 [28.48%]\n",
      "Train epoch 0: Loss: 0.08072461187839508, 11456 / 39995 [28.64%]\n",
      "Train epoch 0: Loss: 0.1266396939754486, 11520 / 39995 [28.8%]\n",
      "Train epoch 0: Loss: 0.054363761097192764, 11584 / 39995 [28.96%]\n",
      "Train epoch 0: Loss: 0.10155908018350601, 11648 / 39995 [29.12%]\n",
      "Train epoch 0: Loss: 0.06342551112174988, 11712 / 39995 [29.28%]\n",
      "Train epoch 0: Loss: 0.09984581172466278, 11776 / 39995 [29.44%]\n",
      "Train epoch 0: Loss: 0.16426900029182434, 11840 / 39995 [29.6%]\n",
      "Train epoch 0: Loss: 0.05526980757713318, 11904 / 39995 [29.76%]\n",
      "Train epoch 0: Loss: 0.0628853291273117, 11968 / 39995 [29.92%]\n",
      "Train epoch 0: Loss: 0.10061980783939362, 12032 / 39995 [30.08%]\n",
      "Train epoch 0: Loss: 0.09148958325386047, 12096 / 39995 [30.24%]\n",
      "Train epoch 0: Loss: 0.05923967808485031, 12160 / 39995 [30.4%]\n",
      "Train epoch 0: Loss: 0.23048771917819977, 12224 / 39995 [30.56%]\n",
      "Train epoch 0: Loss: 0.058315545320510864, 12288 / 39995 [30.72%]\n",
      "Train epoch 0: Loss: 0.04547705873847008, 12352 / 39995 [30.88%]\n",
      "Train epoch 0: Loss: 0.08967746794223785, 12416 / 39995 [31.04%]\n",
      "Train epoch 0: Loss: 0.123638816177845, 12480 / 39995 [31.2%]\n",
      "Train epoch 0: Loss: 0.0751490592956543, 12544 / 39995 [31.36%]\n",
      "Train epoch 0: Loss: 0.07027777284383774, 12608 / 39995 [31.52%]\n",
      "Train epoch 0: Loss: 0.08562733978033066, 12672 / 39995 [31.68%]\n",
      "Train epoch 0: Loss: 0.08635175228118896, 12736 / 39995 [31.84%]\n",
      "Train epoch 0: Loss: 0.11775624752044678, 12800 / 39995 [32.0%]\n",
      "Train epoch 0: Loss: 0.07360232621431351, 12864 / 39995 [32.16%]\n",
      "Train epoch 0: Loss: 0.09630975127220154, 12928 / 39995 [32.32%]\n",
      "Train epoch 0: Loss: 0.06366439163684845, 12992 / 39995 [32.48%]\n",
      "Train epoch 0: Loss: 0.0474739633500576, 13056 / 39995 [32.64%]\n",
      "Train epoch 0: Loss: 0.035340048372745514, 13120 / 39995 [32.8%]\n",
      "Train epoch 0: Loss: 0.07622786611318588, 13184 / 39995 [32.96%]\n",
      "Train epoch 0: Loss: 0.10594593733549118, 13248 / 39995 [33.12%]\n",
      "Train epoch 0: Loss: 0.12097859382629395, 13312 / 39995 [33.28%]\n",
      "Train epoch 0: Loss: 0.07024206221103668, 13376 / 39995 [33.44%]\n",
      "Train epoch 0: Loss: 0.09671975672245026, 13440 / 39995 [33.6%]\n",
      "Train epoch 0: Loss: 0.14577916264533997, 13504 / 39995 [33.76%]\n",
      "Train epoch 0: Loss: 0.0665627121925354, 13568 / 39995 [33.92%]\n",
      "Train epoch 0: Loss: 0.04715132340788841, 13632 / 39995 [34.08%]\n",
      "Train epoch 0: Loss: 0.1643032282590866, 13696 / 39995 [34.24%]\n",
      "Train epoch 0: Loss: 0.08629382401704788, 13760 / 39995 [34.4%]\n",
      "Train epoch 0: Loss: 0.09155125916004181, 13824 / 39995 [34.56%]\n",
      "Train epoch 0: Loss: 0.047883592545986176, 13888 / 39995 [34.72%]\n",
      "Train epoch 0: Loss: 0.0706937164068222, 13952 / 39995 [34.88%]\n",
      "Train epoch 0: Loss: 0.05747814476490021, 14016 / 39995 [35.04%]\n",
      "Train epoch 0: Loss: 0.04460792616009712, 14080 / 39995 [35.2%]\n",
      "Train epoch 0: Loss: 0.14926224946975708, 14144 / 39995 [35.36%]\n",
      "Train epoch 0: Loss: 0.04505245015025139, 14208 / 39995 [35.52%]\n",
      "Train epoch 0: Loss: 0.05227086320519447, 14272 / 39995 [35.68%]\n",
      "Train epoch 0: Loss: 0.08449903130531311, 14336 / 39995 [35.84%]\n",
      "Train epoch 0: Loss: 0.13172873854637146, 14400 / 39995 [36.0%]\n",
      "Train epoch 0: Loss: 0.15847420692443848, 14464 / 39995 [36.16%]\n",
      "Train epoch 0: Loss: 0.11169488728046417, 14528 / 39995 [36.32%]\n",
      "Train epoch 0: Loss: 0.0685625895857811, 14592 / 39995 [36.48%]\n",
      "Train epoch 0: Loss: 0.044345781207084656, 14656 / 39995 [36.64%]\n",
      "Train epoch 0: Loss: 0.06872528046369553, 14720 / 39995 [36.8%]\n",
      "Train epoch 0: Loss: 0.10498042404651642, 14784 / 39995 [36.96%]\n",
      "Train epoch 0: Loss: 0.07042528688907623, 14848 / 39995 [37.12%]\n",
      "Train epoch 0: Loss: 0.05783338472247124, 14912 / 39995 [37.28%]\n",
      "Train epoch 0: Loss: 0.04659818485379219, 14976 / 39995 [37.44%]\n",
      "Train epoch 0: Loss: 0.07066728919744492, 15040 / 39995 [37.6%]\n",
      "Train epoch 0: Loss: 0.06047726050019264, 15104 / 39995 [37.76%]\n",
      "Train epoch 0: Loss: 0.07297445088624954, 15168 / 39995 [37.92%]\n",
      "Train epoch 0: Loss: 0.06772288680076599, 15232 / 39995 [38.08%]\n",
      "Train epoch 0: Loss: 0.05102350935339928, 15296 / 39995 [38.24%]\n",
      "Train epoch 0: Loss: 0.04554462060332298, 15360 / 39995 [38.4%]\n",
      "Train epoch 0: Loss: 0.07357114553451538, 15424 / 39995 [38.56%]\n",
      "Train epoch 0: Loss: 0.05705738440155983, 15488 / 39995 [38.72%]\n",
      "Train epoch 0: Loss: 0.036767907440662384, 15552 / 39995 [38.88%]\n",
      "Train epoch 0: Loss: 0.12336074560880661, 15616 / 39995 [39.04%]\n",
      "Train epoch 0: Loss: 0.06800210475921631, 15680 / 39995 [39.2%]\n",
      "Train epoch 0: Loss: 0.06112261116504669, 15744 / 39995 [39.36%]\n",
      "Train epoch 0: Loss: 0.07160672545433044, 15808 / 39995 [39.52%]\n",
      "Train epoch 0: Loss: 0.07057490944862366, 15872 / 39995 [39.68%]\n",
      "Train epoch 0: Loss: 0.11957093328237534, 15936 / 39995 [39.84%]\n",
      "Train epoch 0: Loss: 0.14392897486686707, 16000 / 39995 [40.0%]\n",
      "Train epoch 0: Loss: 0.043616872280836105, 16064 / 39995 [40.16%]\n",
      "Train epoch 0: Loss: 0.08142179995775223, 16128 / 39995 [40.32%]\n",
      "Train epoch 0: Loss: 0.06777063012123108, 16192 / 39995 [40.48%]\n",
      "Train epoch 0: Loss: 0.05546132102608681, 16256 / 39995 [40.64%]\n",
      "Train epoch 0: Loss: 0.04090672731399536, 16320 / 39995 [40.8%]\n",
      "Train epoch 0: Loss: 0.0752749890089035, 16384 / 39995 [40.96%]\n",
      "Train epoch 0: Loss: 0.1378525197505951, 16448 / 39995 [41.12%]\n",
      "Train epoch 0: Loss: 0.0315549336373806, 16512 / 39995 [41.28%]\n",
      "Train epoch 0: Loss: 0.05690819025039673, 16576 / 39995 [41.44%]\n",
      "Train epoch 0: Loss: 0.07266739010810852, 16640 / 39995 [41.6%]\n",
      "Train epoch 0: Loss: 0.07687251269817352, 16704 / 39995 [41.76%]\n",
      "Train epoch 0: Loss: 0.28138330578804016, 16768 / 39995 [41.92%]\n",
      "Train epoch 0: Loss: 0.05695810168981552, 16832 / 39995 [42.08%]\n",
      "Train epoch 0: Loss: 0.056847915053367615, 16896 / 39995 [42.24%]\n",
      "Train epoch 0: Loss: 0.05579569190740585, 16960 / 39995 [42.4%]\n",
      "Train epoch 0: Loss: 0.10786481201648712, 17024 / 39995 [42.56%]\n",
      "Train epoch 0: Loss: 0.0908798798918724, 17088 / 39995 [42.72%]\n",
      "Train epoch 0: Loss: 0.17058898508548737, 17152 / 39995 [42.88%]\n",
      "Train epoch 0: Loss: 0.14802329242229462, 17216 / 39995 [43.04%]\n",
      "Train epoch 0: Loss: 0.12962296605110168, 17280 / 39995 [43.2%]\n",
      "Train epoch 0: Loss: 0.05782467871904373, 17344 / 39995 [43.36%]\n",
      "Train epoch 0: Loss: 0.06412336230278015, 17408 / 39995 [43.52%]\n",
      "Train epoch 0: Loss: 0.04473739489912987, 17472 / 39995 [43.68%]\n",
      "Train epoch 0: Loss: 0.0881771519780159, 17536 / 39995 [43.84%]\n",
      "Train epoch 0: Loss: 0.04013443738222122, 17600 / 39995 [44.0%]\n",
      "Train epoch 0: Loss: 0.024740345776081085, 17664 / 39995 [44.16%]\n",
      "Train epoch 0: Loss: 0.07646298408508301, 17728 / 39995 [44.32%]\n",
      "Train epoch 0: Loss: 0.10799767076969147, 17792 / 39995 [44.48%]\n",
      "Train epoch 0: Loss: 0.08232804387807846, 17856 / 39995 [44.64%]\n",
      "Train epoch 0: Loss: 0.07553378492593765, 17920 / 39995 [44.8%]\n",
      "Train epoch 0: Loss: 0.08995908498764038, 17984 / 39995 [44.96%]\n",
      "Train epoch 0: Loss: 0.04251100867986679, 18048 / 39995 [45.12%]\n",
      "Train epoch 0: Loss: 0.051859818398952484, 18112 / 39995 [45.28%]\n",
      "Train epoch 0: Loss: 0.052936941385269165, 18176 / 39995 [45.44%]\n",
      "Train epoch 0: Loss: 0.030054936185479164, 18240 / 39995 [45.6%]\n",
      "Train epoch 0: Loss: 0.03857439383864403, 18304 / 39995 [45.76%]\n",
      "Train epoch 0: Loss: 0.10567547380924225, 18368 / 39995 [45.92%]\n",
      "Train epoch 0: Loss: 0.04666333273053169, 18432 / 39995 [46.08%]\n",
      "Train epoch 0: Loss: 0.05544396862387657, 18496 / 39995 [46.24%]\n",
      "Train epoch 0: Loss: 0.04382607713341713, 18560 / 39995 [46.4%]\n",
      "Train epoch 0: Loss: 0.06585145741701126, 18624 / 39995 [46.56%]\n",
      "Train epoch 0: Loss: 0.08236084878444672, 18688 / 39995 [46.72%]\n",
      "Train epoch 0: Loss: 0.0655154138803482, 18752 / 39995 [46.88%]\n",
      "Train epoch 0: Loss: 0.14847029745578766, 18816 / 39995 [47.04%]\n",
      "Train epoch 0: Loss: 0.06184772402048111, 18880 / 39995 [47.2%]\n",
      "Train epoch 0: Loss: 0.11375629901885986, 18944 / 39995 [47.36%]\n",
      "Train epoch 0: Loss: 0.021533885970711708, 19008 / 39995 [47.52%]\n",
      "Train epoch 0: Loss: 0.0549800843000412, 19072 / 39995 [47.68%]\n",
      "Train epoch 0: Loss: 0.030217932537198067, 19136 / 39995 [47.84%]\n",
      "Train epoch 0: Loss: 0.07658818364143372, 19200 / 39995 [48.0%]\n",
      "Train epoch 0: Loss: 0.11622089892625809, 19264 / 39995 [48.16%]\n",
      "Train epoch 0: Loss: 0.06658335030078888, 19328 / 39995 [48.32%]\n",
      "Train epoch 0: Loss: 0.08017754554748535, 19392 / 39995 [48.48%]\n",
      "Train epoch 0: Loss: 0.06583763659000397, 19456 / 39995 [48.64%]\n",
      "Train epoch 0: Loss: 0.06265681982040405, 19520 / 39995 [48.8%]\n",
      "Train epoch 0: Loss: 0.05118170753121376, 19584 / 39995 [48.96%]\n",
      "Train epoch 0: Loss: 0.05453537777066231, 19648 / 39995 [49.12%]\n",
      "Train epoch 0: Loss: 0.04470448940992355, 19712 / 39995 [49.28%]\n",
      "Train epoch 0: Loss: 0.12679018080234528, 19776 / 39995 [49.44%]\n",
      "Train epoch 0: Loss: 0.08985034376382828, 19840 / 39995 [49.6%]\n",
      "Train epoch 0: Loss: 0.09918029606342316, 19904 / 39995 [49.76%]\n",
      "Train epoch 0: Loss: 0.020476289093494415, 19968 / 39995 [49.92%]\n",
      "Train epoch 0: Loss: 0.05184217169880867, 20032 / 39995 [50.08%]\n",
      "Train epoch 0: Loss: 0.04601413011550903, 20096 / 39995 [50.24%]\n",
      "Train epoch 0: Loss: 0.08333452045917511, 20160 / 39995 [50.4%]\n",
      "Train epoch 0: Loss: 0.03322218731045723, 20224 / 39995 [50.56%]\n",
      "Train epoch 0: Loss: 0.1247393786907196, 20288 / 39995 [50.72%]\n",
      "Train epoch 0: Loss: 0.10792165994644165, 20352 / 39995 [50.88%]\n",
      "Train epoch 0: Loss: 0.0749775692820549, 20416 / 39995 [51.04%]\n",
      "Train epoch 0: Loss: 0.03160946071147919, 20480 / 39995 [51.2%]\n",
      "Train epoch 0: Loss: 0.03187355399131775, 20544 / 39995 [51.36%]\n",
      "Train epoch 0: Loss: 0.08844958245754242, 20608 / 39995 [51.52%]\n",
      "Train epoch 0: Loss: 0.03110223077237606, 20672 / 39995 [51.68%]\n",
      "Train epoch 0: Loss: 0.05868345871567726, 20736 / 39995 [51.84%]\n",
      "Train epoch 0: Loss: 0.04531462490558624, 20800 / 39995 [52.0%]\n",
      "Train epoch 0: Loss: 0.03031984716653824, 20864 / 39995 [52.16%]\n",
      "Train epoch 0: Loss: 0.10011087357997894, 20928 / 39995 [52.32%]\n",
      "Train epoch 0: Loss: 0.020194705575704575, 20992 / 39995 [52.48%]\n",
      "Train epoch 0: Loss: 0.04822314903140068, 21056 / 39995 [52.64%]\n",
      "Train epoch 0: Loss: 0.028314432129263878, 21120 / 39995 [52.8%]\n",
      "Train epoch 0: Loss: 0.03732398897409439, 21184 / 39995 [52.96%]\n",
      "Train epoch 0: Loss: 0.07811175286769867, 21248 / 39995 [53.12%]\n",
      "Train epoch 0: Loss: 0.1513223648071289, 21312 / 39995 [53.28%]\n",
      "Train epoch 0: Loss: 0.06192595139145851, 21376 / 39995 [53.44%]\n",
      "Train epoch 0: Loss: 0.046545740216970444, 21440 / 39995 [53.6%]\n",
      "Train epoch 0: Loss: 0.03837195783853531, 21504 / 39995 [53.76%]\n",
      "Train epoch 0: Loss: 0.12173612415790558, 21568 / 39995 [53.92%]\n",
      "Train epoch 0: Loss: 0.06661023944616318, 21632 / 39995 [54.08%]\n",
      "Train epoch 0: Loss: 0.0668647438287735, 21696 / 39995 [54.24%]\n",
      "Train epoch 0: Loss: 0.02397022396326065, 21760 / 39995 [54.4%]\n",
      "Train epoch 0: Loss: 0.07071659713983536, 21824 / 39995 [54.56%]\n",
      "Train epoch 0: Loss: 0.04202824458479881, 21888 / 39995 [54.72%]\n",
      "Train epoch 0: Loss: 0.06598640978336334, 21952 / 39995 [54.88%]\n",
      "Train epoch 0: Loss: 0.02662348374724388, 22016 / 39995 [55.04%]\n",
      "Train epoch 0: Loss: 0.04754955321550369, 22080 / 39995 [55.2%]\n",
      "Train epoch 0: Loss: 0.060119617730379105, 22144 / 39995 [55.36%]\n",
      "Train epoch 0: Loss: 0.022027742117643356, 22208 / 39995 [55.52%]\n",
      "Train epoch 0: Loss: 0.09879244863986969, 22272 / 39995 [55.68%]\n",
      "Train epoch 0: Loss: 0.11563117802143097, 22336 / 39995 [55.84%]\n",
      "Train epoch 0: Loss: 0.04144401103258133, 22400 / 39995 [56.0%]\n",
      "Train epoch 0: Loss: 0.02906039170920849, 22464 / 39995 [56.16%]\n",
      "Train epoch 0: Loss: 0.030576661229133606, 22528 / 39995 [56.32%]\n",
      "Train epoch 0: Loss: 0.09927990287542343, 22592 / 39995 [56.48%]\n",
      "Train epoch 0: Loss: 0.06865960359573364, 22656 / 39995 [56.64%]\n",
      "Train epoch 0: Loss: 0.03797769173979759, 22720 / 39995 [56.8%]\n",
      "Train epoch 0: Loss: 0.05298436060547829, 22784 / 39995 [56.96%]\n",
      "Train epoch 0: Loss: 0.11031779646873474, 22848 / 39995 [57.12%]\n",
      "Train epoch 0: Loss: 0.1372023969888687, 22912 / 39995 [57.28%]\n",
      "Train epoch 0: Loss: 0.05005456507205963, 22976 / 39995 [57.44%]\n",
      "Train epoch 0: Loss: 0.08769520372152328, 23040 / 39995 [57.6%]\n",
      "Train epoch 0: Loss: 0.04100195690989494, 23104 / 39995 [57.76%]\n",
      "Train epoch 0: Loss: 0.03742724284529686, 23168 / 39995 [57.92%]\n",
      "Train epoch 0: Loss: 0.0574975349009037, 23232 / 39995 [58.08%]\n",
      "Train epoch 0: Loss: 0.025263270363211632, 23296 / 39995 [58.24%]\n",
      "Train epoch 0: Loss: 0.02089662477374077, 23360 / 39995 [58.4%]\n",
      "Train epoch 0: Loss: 0.050044406205415726, 23424 / 39995 [58.56%]\n",
      "Train epoch 0: Loss: 0.023647595196962357, 23488 / 39995 [58.72%]\n",
      "Train epoch 0: Loss: 0.0706324428319931, 23552 / 39995 [58.88%]\n",
      "Train epoch 0: Loss: 0.116828054189682, 23616 / 39995 [59.04%]\n",
      "Train epoch 0: Loss: 0.04125462844967842, 23680 / 39995 [59.2%]\n",
      "Train epoch 0: Loss: 0.03621647134423256, 23744 / 39995 [59.36%]\n",
      "Train epoch 0: Loss: 0.022948993369936943, 23808 / 39995 [59.52%]\n",
      "Train epoch 0: Loss: 0.033515624701976776, 23872 / 39995 [59.68%]\n",
      "Train epoch 0: Loss: 0.05807773768901825, 23936 / 39995 [59.84%]\n",
      "Train epoch 0: Loss: 0.0858566015958786, 24000 / 39995 [60.0%]\n",
      "Train epoch 0: Loss: 0.03139658272266388, 24064 / 39995 [60.16%]\n",
      "Train epoch 0: Loss: 0.041199974715709686, 24128 / 39995 [60.32%]\n",
      "Train epoch 0: Loss: 0.049517855048179626, 24192 / 39995 [60.48%]\n",
      "Train epoch 0: Loss: 0.11752311140298843, 24256 / 39995 [60.64%]\n",
      "Train epoch 0: Loss: 0.08227205276489258, 24320 / 39995 [60.8%]\n",
      "Train epoch 0: Loss: 0.0313403457403183, 24384 / 39995 [60.96%]\n",
      "Train epoch 0: Loss: 0.047426581382751465, 24448 / 39995 [61.12%]\n",
      "Train epoch 0: Loss: 0.014721107669174671, 24512 / 39995 [61.28%]\n",
      "Train epoch 0: Loss: 0.04483111575245857, 24576 / 39995 [61.44%]\n",
      "Train epoch 0: Loss: 0.02767617627978325, 24640 / 39995 [61.6%]\n",
      "Train epoch 0: Loss: 0.060832373797893524, 24704 / 39995 [61.76%]\n",
      "Train epoch 0: Loss: 0.02729954570531845, 24768 / 39995 [61.92%]\n",
      "Train epoch 0: Loss: 0.03792734816670418, 24832 / 39995 [62.08%]\n",
      "Train epoch 0: Loss: 0.10234170407056808, 24896 / 39995 [62.24%]\n",
      "Train epoch 0: Loss: 0.06363841891288757, 24960 / 39995 [62.4%]\n",
      "Train epoch 0: Loss: 0.0710831731557846, 25024 / 39995 [62.56%]\n",
      "Train epoch 0: Loss: 0.03731097653508186, 25088 / 39995 [62.72%]\n",
      "Train epoch 0: Loss: 0.10583585500717163, 25152 / 39995 [62.88%]\n",
      "Train epoch 0: Loss: 0.06672628968954086, 25216 / 39995 [63.04%]\n",
      "Train epoch 0: Loss: 0.0354924350976944, 25280 / 39995 [63.2%]\n",
      "Train epoch 0: Loss: 0.05996004864573479, 25344 / 39995 [63.36%]\n",
      "Train epoch 0: Loss: 0.08420496433973312, 25408 / 39995 [63.52%]\n",
      "Train epoch 0: Loss: 0.022884603589773178, 25472 / 39995 [63.68%]\n",
      "Train epoch 0: Loss: 0.08839866518974304, 25536 / 39995 [63.84%]\n",
      "Train epoch 0: Loss: 0.03639897331595421, 25600 / 39995 [64.0%]\n",
      "Train epoch 0: Loss: 0.07695365697145462, 25664 / 39995 [64.16%]\n",
      "Train epoch 0: Loss: 0.041729044169187546, 25728 / 39995 [64.32%]\n",
      "Train epoch 0: Loss: 0.1193433403968811, 25792 / 39995 [64.48%]\n",
      "Train epoch 0: Loss: 0.06915292143821716, 25856 / 39995 [64.64%]\n",
      "Train epoch 0: Loss: 0.05763750150799751, 25920 / 39995 [64.8%]\n",
      "Train epoch 0: Loss: 0.06420192122459412, 25984 / 39995 [64.96%]\n",
      "Train epoch 0: Loss: 0.20360122621059418, 26048 / 39995 [65.12%]\n",
      "Train epoch 0: Loss: 0.05690772831439972, 26112 / 39995 [65.28%]\n",
      "Train epoch 0: Loss: 0.022289825603365898, 26176 / 39995 [65.44%]\n",
      "Train epoch 0: Loss: 0.026806440204381943, 26240 / 39995 [65.6%]\n",
      "Train epoch 0: Loss: 0.06028864532709122, 26304 / 39995 [65.76%]\n",
      "Train epoch 0: Loss: 0.02098551020026207, 26368 / 39995 [65.92%]\n",
      "Train epoch 0: Loss: 0.02641696110367775, 26432 / 39995 [66.08%]\n",
      "Train epoch 0: Loss: 0.03375342860817909, 26496 / 39995 [66.24%]\n",
      "Train epoch 0: Loss: 0.036449603736400604, 26560 / 39995 [66.4%]\n",
      "Train epoch 0: Loss: 0.030874276533722878, 26624 / 39995 [66.56%]\n",
      "Train epoch 0: Loss: 0.047969162464141846, 26688 / 39995 [66.72%]\n",
      "Train epoch 0: Loss: 0.03518715128302574, 26752 / 39995 [66.88%]\n",
      "Train epoch 0: Loss: 0.05114161968231201, 26816 / 39995 [67.04%]\n",
      "Train epoch 0: Loss: 0.055474307388067245, 26880 / 39995 [67.2%]\n",
      "Train epoch 0: Loss: 0.1390579789876938, 26944 / 39995 [67.36%]\n",
      "Train epoch 0: Loss: 0.043423984199762344, 27008 / 39995 [67.52%]\n",
      "Train epoch 0: Loss: 0.18815656006336212, 27072 / 39995 [67.68%]\n",
      "Train epoch 0: Loss: 0.11768606305122375, 27136 / 39995 [67.84%]\n",
      "Train epoch 0: Loss: 0.07151047885417938, 27200 / 39995 [68.0%]\n",
      "Train epoch 0: Loss: 0.059120241552591324, 27264 / 39995 [68.16%]\n",
      "Train epoch 0: Loss: 0.03875407949090004, 27328 / 39995 [68.32%]\n",
      "Train epoch 0: Loss: 0.1355593204498291, 27392 / 39995 [68.48%]\n",
      "Train epoch 0: Loss: 0.08299955725669861, 27456 / 39995 [68.64%]\n",
      "Train epoch 0: Loss: 0.025203758850693703, 27520 / 39995 [68.8%]\n",
      "Train epoch 0: Loss: 0.03929685428738594, 27584 / 39995 [68.96%]\n",
      "Train epoch 0: Loss: 0.06509511917829514, 27648 / 39995 [69.12%]\n",
      "Train epoch 0: Loss: 0.08905362337827682, 27712 / 39995 [69.28%]\n",
      "Train epoch 0: Loss: 0.06782617419958115, 27776 / 39995 [69.44%]\n",
      "Train epoch 0: Loss: 0.0395861491560936, 27840 / 39995 [69.6%]\n",
      "Train epoch 0: Loss: 0.07867840677499771, 27904 / 39995 [69.76%]\n",
      "Train epoch 0: Loss: 0.05824825167655945, 27968 / 39995 [69.92%]\n",
      "Train epoch 0: Loss: 0.024927228689193726, 28032 / 39995 [70.08%]\n",
      "Train epoch 0: Loss: 0.032875802367925644, 28096 / 39995 [70.24%]\n",
      "Train epoch 0: Loss: 0.11776500195264816, 28160 / 39995 [70.4%]\n",
      "Train epoch 0: Loss: 0.03523918241262436, 28224 / 39995 [70.56%]\n",
      "Train epoch 0: Loss: 0.02449297159910202, 28288 / 39995 [70.72%]\n",
      "Train epoch 0: Loss: 0.028506208211183548, 28352 / 39995 [70.88%]\n",
      "Train epoch 0: Loss: 0.08149157464504242, 28416 / 39995 [71.04%]\n",
      "Train epoch 0: Loss: 0.19462807476520538, 28480 / 39995 [71.2%]\n",
      "Train epoch 0: Loss: 0.03975917771458626, 28544 / 39995 [71.36%]\n",
      "Train epoch 0: Loss: 0.023218829184770584, 28608 / 39995 [71.52%]\n",
      "Train epoch 0: Loss: 0.019052382558584213, 28672 / 39995 [71.68%]\n",
      "Train epoch 0: Loss: 0.014617059379816055, 28736 / 39995 [71.84%]\n",
      "Train epoch 0: Loss: 0.0244820024818182, 28800 / 39995 [72.0%]\n",
      "Train epoch 0: Loss: 0.05069233849644661, 28864 / 39995 [72.16%]\n",
      "Train epoch 0: Loss: 0.012586128897964954, 28928 / 39995 [72.32%]\n",
      "Train epoch 0: Loss: 0.038340430706739426, 28992 / 39995 [72.48%]\n",
      "Train epoch 0: Loss: 0.014868460595607758, 29056 / 39995 [72.64%]\n",
      "Train epoch 0: Loss: 0.07002706825733185, 29120 / 39995 [72.8%]\n",
      "Train epoch 0: Loss: 0.016529520973563194, 29184 / 39995 [72.96%]\n",
      "Train epoch 0: Loss: 0.04822501167654991, 29248 / 39995 [73.12%]\n",
      "Train epoch 0: Loss: 0.024394404143095016, 29312 / 39995 [73.28%]\n",
      "Train epoch 0: Loss: 0.0537608303129673, 29376 / 39995 [73.44%]\n",
      "Train epoch 0: Loss: 0.017245043069124222, 29440 / 39995 [73.6%]\n",
      "Train epoch 0: Loss: 0.07669602334499359, 29504 / 39995 [73.76%]\n",
      "Train epoch 0: Loss: 0.017269255593419075, 29568 / 39995 [73.92%]\n",
      "Train epoch 0: Loss: 0.04091843590140343, 29632 / 39995 [74.08%]\n",
      "Train epoch 0: Loss: 0.03422214090824127, 29696 / 39995 [74.24%]\n",
      "Train epoch 0: Loss: 0.07580097019672394, 29760 / 39995 [74.4%]\n",
      "Train epoch 0: Loss: 0.03147733956575394, 29824 / 39995 [74.56%]\n",
      "Train epoch 0: Loss: 0.015089158900082111, 29888 / 39995 [74.72%]\n",
      "Train epoch 0: Loss: 0.027783522382378578, 29952 / 39995 [74.88%]\n",
      "Train epoch 0: Loss: 0.046400297433137894, 30016 / 39995 [75.04%]\n",
      "Train epoch 0: Loss: 0.018689798191189766, 30080 / 39995 [75.2%]\n",
      "Train epoch 0: Loss: 0.025787921622395515, 30144 / 39995 [75.36%]\n",
      "Train epoch 0: Loss: 0.16527490317821503, 30208 / 39995 [75.52%]\n",
      "Train epoch 0: Loss: 0.014468491077423096, 30272 / 39995 [75.68%]\n",
      "Train epoch 0: Loss: 0.047064408659935, 30336 / 39995 [75.84%]\n",
      "Train epoch 0: Loss: 0.017184285447001457, 30400 / 39995 [76.0%]\n",
      "Train epoch 0: Loss: 0.08970809727907181, 30464 / 39995 [76.16%]\n",
      "Train epoch 0: Loss: 0.0789523497223854, 30528 / 39995 [76.32%]\n",
      "Train epoch 0: Loss: 0.16489875316619873, 30592 / 39995 [76.48%]\n",
      "Train epoch 0: Loss: 0.01736680045723915, 30656 / 39995 [76.64%]\n",
      "Train epoch 0: Loss: 0.013928514905273914, 30720 / 39995 [76.8%]\n",
      "Train epoch 0: Loss: 0.155061274766922, 30784 / 39995 [76.96%]\n",
      "Train epoch 0: Loss: 0.023406291380524635, 30848 / 39995 [77.12%]\n",
      "Train epoch 0: Loss: 0.07968099415302277, 30912 / 39995 [77.28%]\n",
      "Train epoch 0: Loss: 0.029338030144572258, 30976 / 39995 [77.44%]\n",
      "Train epoch 0: Loss: 0.12233840674161911, 31040 / 39995 [77.6%]\n",
      "Train epoch 0: Loss: 0.08667168021202087, 31104 / 39995 [77.76%]\n",
      "Train epoch 0: Loss: 0.08417759835720062, 31168 / 39995 [77.92%]\n",
      "Train epoch 0: Loss: 0.05181512236595154, 31232 / 39995 [78.08%]\n",
      "Train epoch 0: Loss: 0.03925235569477081, 31296 / 39995 [78.24%]\n",
      "Train epoch 0: Loss: 0.0155851561576128, 31360 / 39995 [78.4%]\n",
      "Train epoch 0: Loss: 0.16622032225131989, 31424 / 39995 [78.56%]\n",
      "Train epoch 0: Loss: 0.020436957478523254, 31488 / 39995 [78.72%]\n",
      "Train epoch 0: Loss: 0.15266576409339905, 31552 / 39995 [78.88%]\n",
      "Train epoch 0: Loss: 0.0298882108181715, 31616 / 39995 [79.04%]\n",
      "Train epoch 0: Loss: 0.08562654256820679, 31680 / 39995 [79.2%]\n",
      "Train epoch 0: Loss: 0.03682945668697357, 31744 / 39995 [79.36%]\n",
      "Train epoch 0: Loss: 0.03410081937909126, 31808 / 39995 [79.52%]\n",
      "Train epoch 0: Loss: 0.022832240909337997, 31872 / 39995 [79.68%]\n",
      "Train epoch 0: Loss: 0.025992069393396378, 31936 / 39995 [79.84%]\n",
      "Train epoch 0: Loss: 0.08907600492238998, 32000 / 39995 [80.0%]\n",
      "Train epoch 0: Loss: 0.038393136113882065, 32064 / 39995 [80.16%]\n",
      "Train epoch 0: Loss: 0.07879290729761124, 32128 / 39995 [80.32%]\n",
      "Train epoch 0: Loss: 0.038879234343767166, 32192 / 39995 [80.48%]\n",
      "Train epoch 0: Loss: 0.15919837355613708, 32256 / 39995 [80.64%]\n",
      "Train epoch 0: Loss: 0.045448243618011475, 32320 / 39995 [80.8%]\n",
      "Train epoch 0: Loss: 0.055427417159080505, 32384 / 39995 [80.96%]\n",
      "Train epoch 0: Loss: 0.028473418205976486, 32448 / 39995 [81.12%]\n",
      "Train epoch 0: Loss: 0.024223076179623604, 32512 / 39995 [81.28%]\n",
      "Train epoch 0: Loss: 0.0725279226899147, 32576 / 39995 [81.44%]\n",
      "Train epoch 0: Loss: 0.09535810351371765, 32640 / 39995 [81.6%]\n",
      "Train epoch 0: Loss: 0.050571806728839874, 32704 / 39995 [81.76%]\n",
      "Train epoch 0: Loss: 0.02393924817442894, 32768 / 39995 [81.92%]\n",
      "Train epoch 0: Loss: 0.026611732318997383, 32832 / 39995 [82.08%]\n",
      "Train epoch 0: Loss: 0.04112207144498825, 32896 / 39995 [82.24%]\n",
      "Train epoch 0: Loss: 0.04177547246217728, 32960 / 39995 [82.4%]\n",
      "Train epoch 0: Loss: 0.08469005674123764, 33024 / 39995 [82.56%]\n",
      "Train epoch 0: Loss: 0.06388591974973679, 33088 / 39995 [82.72%]\n",
      "Train epoch 0: Loss: 0.07888980954885483, 33152 / 39995 [82.88%]\n",
      "Train epoch 0: Loss: 0.05907373130321503, 33216 / 39995 [83.04%]\n",
      "Train epoch 0: Loss: 0.013648800551891327, 33280 / 39995 [83.2%]\n",
      "Train epoch 0: Loss: 0.08492215722799301, 33344 / 39995 [83.36%]\n",
      "Train epoch 0: Loss: 0.018430709838867188, 33408 / 39995 [83.52%]\n",
      "Train epoch 0: Loss: 0.012726734392344952, 33472 / 39995 [83.68%]\n",
      "Train epoch 0: Loss: 0.08421088010072708, 33536 / 39995 [83.84%]\n",
      "Train epoch 0: Loss: 0.05243420973420143, 33600 / 39995 [84.0%]\n",
      "Train epoch 0: Loss: 0.06269960105419159, 33664 / 39995 [84.16%]\n",
      "Train epoch 0: Loss: 0.03167112171649933, 33728 / 39995 [84.32%]\n",
      "Train epoch 0: Loss: 0.03801395744085312, 33792 / 39995 [84.48%]\n",
      "Train epoch 0: Loss: 0.04026862606406212, 33856 / 39995 [84.64%]\n",
      "Train epoch 0: Loss: 0.08587893098592758, 33920 / 39995 [84.8%]\n",
      "Train epoch 0: Loss: 0.08623196184635162, 33984 / 39995 [84.96%]\n",
      "Train epoch 0: Loss: 0.018653081730008125, 34048 / 39995 [85.12%]\n",
      "Train epoch 0: Loss: 0.047030117362737656, 34112 / 39995 [85.28%]\n",
      "Train epoch 0: Loss: 0.01799248717725277, 34176 / 39995 [85.44%]\n",
      "Train epoch 0: Loss: 0.019798891618847847, 34240 / 39995 [85.6%]\n",
      "Train epoch 0: Loss: 0.05587794631719589, 34304 / 39995 [85.76%]\n",
      "Train epoch 0: Loss: 0.027570724487304688, 34368 / 39995 [85.92%]\n",
      "Train epoch 0: Loss: 0.10065526515245438, 34432 / 39995 [86.08%]\n",
      "Train epoch 0: Loss: 0.16659778356552124, 34496 / 39995 [86.24%]\n",
      "Train epoch 0: Loss: 0.09355650842189789, 34560 / 39995 [86.4%]\n",
      "Train epoch 0: Loss: 0.012537690810859203, 34624 / 39995 [86.56%]\n",
      "Train epoch 0: Loss: 0.026921648532152176, 34688 / 39995 [86.72%]\n",
      "Train epoch 0: Loss: 0.02298777922987938, 34752 / 39995 [86.88%]\n",
      "Train epoch 0: Loss: 0.1315305531024933, 34816 / 39995 [87.04%]\n",
      "Train epoch 0: Loss: 0.07689791917800903, 34880 / 39995 [87.2%]\n",
      "Train epoch 0: Loss: 0.02323829010128975, 34944 / 39995 [87.36%]\n",
      "Train epoch 0: Loss: 0.09353483468294144, 35008 / 39995 [87.52%]\n",
      "Train epoch 0: Loss: 0.04428291693329811, 35072 / 39995 [87.68%]\n",
      "Train epoch 0: Loss: 0.05343662574887276, 35136 / 39995 [87.84%]\n",
      "Train epoch 0: Loss: 0.020822150632739067, 35200 / 39995 [88.0%]\n",
      "Train epoch 0: Loss: 0.01725364662706852, 35264 / 39995 [88.16%]\n",
      "Train epoch 0: Loss: 0.05949018895626068, 35328 / 39995 [88.32%]\n",
      "Train epoch 0: Loss: 0.0301803145557642, 35392 / 39995 [88.48%]\n",
      "Train epoch 0: Loss: 0.03494521975517273, 35456 / 39995 [88.64%]\n",
      "Train epoch 0: Loss: 0.0753808543086052, 35520 / 39995 [88.8%]\n",
      "Train epoch 0: Loss: 0.24135930836200714, 35584 / 39995 [88.96%]\n",
      "Train epoch 0: Loss: 0.07747747004032135, 35648 / 39995 [89.12%]\n",
      "Train epoch 0: Loss: 0.14102593064308167, 35712 / 39995 [89.28%]\n",
      "Train epoch 0: Loss: 0.19604286551475525, 35776 / 39995 [89.44%]\n",
      "Train epoch 0: Loss: 0.0417674221098423, 35840 / 39995 [89.6%]\n",
      "Train epoch 0: Loss: 0.07404720038175583, 35904 / 39995 [89.76%]\n",
      "Train epoch 0: Loss: 0.025042472407221794, 35968 / 39995 [89.92%]\n",
      "Train epoch 0: Loss: 0.10309068858623505, 36032 / 39995 [90.08%]\n",
      "Train epoch 0: Loss: 0.017678692936897278, 36096 / 39995 [90.24%]\n",
      "Train epoch 0: Loss: 0.04430504888296127, 36160 / 39995 [90.4%]\n",
      "Train epoch 0: Loss: 0.0542343407869339, 36224 / 39995 [90.56%]\n",
      "Train epoch 0: Loss: 0.07675588130950928, 36288 / 39995 [90.72%]\n",
      "Train epoch 0: Loss: 0.01915966533124447, 36352 / 39995 [90.88%]\n",
      "Train epoch 0: Loss: 0.07157879322767258, 36416 / 39995 [91.04%]\n",
      "Train epoch 0: Loss: 0.07463733851909637, 36480 / 39995 [91.2%]\n",
      "Train epoch 0: Loss: 0.04655233398079872, 36544 / 39995 [91.36%]\n",
      "Train epoch 0: Loss: 0.06286001950502396, 36608 / 39995 [91.52%]\n",
      "Train epoch 0: Loss: 0.015369724482297897, 36672 / 39995 [91.68%]\n",
      "Train epoch 0: Loss: 0.019268687814474106, 36736 / 39995 [91.84%]\n",
      "Train epoch 0: Loss: 0.031260475516319275, 36800 / 39995 [92.0%]\n",
      "Train epoch 0: Loss: 0.021747520193457603, 36864 / 39995 [92.16%]\n",
      "Train epoch 0: Loss: 0.06859485059976578, 36928 / 39995 [92.32%]\n",
      "Train epoch 0: Loss: 0.021399274468421936, 36992 / 39995 [92.48%]\n",
      "Train epoch 0: Loss: 0.05281909927725792, 37056 / 39995 [92.64%]\n",
      "Train epoch 0: Loss: 0.06859205663204193, 37120 / 39995 [92.8%]\n",
      "Train epoch 0: Loss: 0.0253459345549345, 37184 / 39995 [92.96%]\n",
      "Train epoch 0: Loss: 0.08144275099039078, 37248 / 39995 [93.12%]\n",
      "Train epoch 0: Loss: 0.0583144947886467, 37312 / 39995 [93.28%]\n",
      "Train epoch 0: Loss: 0.1740317940711975, 37376 / 39995 [93.44%]\n",
      "Train epoch 0: Loss: 0.03066284954547882, 37440 / 39995 [93.6%]\n",
      "Train epoch 0: Loss: 0.14064499735832214, 37504 / 39995 [93.76%]\n",
      "Train epoch 0: Loss: 0.027860799804329872, 37568 / 39995 [93.92%]\n",
      "Train epoch 0: Loss: 0.09076805412769318, 37632 / 39995 [94.08%]\n",
      "Train epoch 0: Loss: 0.0094144307076931, 37696 / 39995 [94.24%]\n",
      "Train epoch 0: Loss: 0.015688413754105568, 37760 / 39995 [94.4%]\n",
      "Train epoch 0: Loss: 0.03405393660068512, 37824 / 39995 [94.56%]\n",
      "Train epoch 0: Loss: 0.14872807264328003, 37888 / 39995 [94.72%]\n",
      "Train epoch 0: Loss: 0.05193854868412018, 37952 / 39995 [94.88%]\n",
      "Train epoch 0: Loss: 0.013987467624247074, 38016 / 39995 [95.04%]\n",
      "Train epoch 0: Loss: 0.011152409017086029, 38080 / 39995 [95.2%]\n",
      "Train epoch 0: Loss: 0.03972921893000603, 38144 / 39995 [95.36%]\n",
      "Train epoch 0: Loss: 0.04514819383621216, 38208 / 39995 [95.52%]\n",
      "Train epoch 0: Loss: 0.07036732882261276, 38272 / 39995 [95.68%]\n",
      "Train epoch 0: Loss: 0.041310589760541916, 38336 / 39995 [95.84%]\n",
      "Train epoch 0: Loss: 0.01199439074844122, 38400 / 39995 [96.0%]\n",
      "Train epoch 0: Loss: 0.05014428496360779, 38464 / 39995 [96.16%]\n",
      "Train epoch 0: Loss: 0.1228235587477684, 38528 / 39995 [96.32%]\n",
      "Train epoch 0: Loss: 0.016871849074959755, 38592 / 39995 [96.48%]\n",
      "Train epoch 0: Loss: 0.17352978885173798, 38656 / 39995 [96.64%]\n",
      "Train epoch 0: Loss: 0.06704023480415344, 38720 / 39995 [96.8%]\n",
      "Train epoch 0: Loss: 0.011829180642962456, 38784 / 39995 [96.96%]\n",
      "Train epoch 0: Loss: 0.05888442322611809, 38848 / 39995 [97.12%]\n",
      "Train epoch 0: Loss: 0.019822612404823303, 38912 / 39995 [97.28%]\n",
      "Train epoch 0: Loss: 0.05344249680638313, 38976 / 39995 [97.44%]\n",
      "Train epoch 0: Loss: 0.07506941258907318, 39040 / 39995 [97.6%]\n",
      "Train epoch 0: Loss: 0.01511395163834095, 39104 / 39995 [97.76%]\n",
      "Train epoch 0: Loss: 0.05322682857513428, 39168 / 39995 [97.92%]\n",
      "Train epoch 0: Loss: 0.02109447307884693, 39232 / 39995 [98.08%]\n",
      "Train epoch 0: Loss: 0.037680305540561676, 39296 / 39995 [98.24%]\n",
      "Train epoch 0: Loss: 0.0827537477016449, 39360 / 39995 [98.4%]\n",
      "Train epoch 0: Loss: 0.019121704623103142, 39424 / 39995 [98.56%]\n",
      "Train epoch 0: Loss: 0.018465356901288033, 39488 / 39995 [98.72%]\n",
      "Train epoch 0: Loss: 0.038437239825725555, 39552 / 39995 [98.88%]\n",
      "Train epoch 0: Loss: 0.04286917299032211, 39616 / 39995 [99.04%]\n",
      "Train epoch 0: Loss: 0.008634636178612709, 39680 / 39995 [99.2%]\n",
      "Train epoch 0: Loss: 0.008911041542887688, 39744 / 39995 [99.36%]\n",
      "Train epoch 0: Loss: 0.10127004981040955, 39808 / 39995 [99.52%]\n",
      "Train epoch 0: Loss: 0.07165879011154175, 39872 / 39995 [99.68%]\n",
      "Train epoch 0: Loss: 0.08671703189611435, 36816 / 39995 [99.84%]\n",
      "Test accuracy:  94.47430928866109\n",
      "\n",
      " Accuracy: tensor(0.9447)\n",
      "+5 points for accuracy above 0.75\n",
      "+5 points for accuracy above 0.8\n",
      "+5 points for accuracy above 0.85\n",
      "+5 points for accuracy above 0.9\n",
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 421.066s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gradescope_utils\n",
    "!python grade.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bf1ce45c78dc10e46902ce2353bf584b4033783ac6e2a7b41a0377a2a16628d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
